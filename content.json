{"pages":[{"title":"","text":"I'm Harrison-1eo An undergraduate student at the School of Cyberspace Security of Beihang University If you have any questions, welcome to contact me via email. 😊😊😊😁😁😁😋😋😋 😎😎😎🤗🤗🤗🤩🤩🤩 😝😝😝🫣🫣🫣🥺🥺🥺","link":"/about/index.html"},{"title":"","text":"Harrison-1eo (Harrison) (github.com)","link":"/links/index.html"}],"posts":[{"title":"python 基础语法","text":"第一章 python简介 第二章 python基础语法 2.1 简单输出 12print(666)print([输出项1,输出项2,...] [,sep=分隔符] [,end=结束符]) 默认分隔符为空格，结束符为换行符 2.2 注释 使用 # 表示单行注释 使用 \"\"\" 表示多行注释 2.3 变量 变量是没有类型的，但是变量存储的数据是有类型的 可以使用 type() 查看变量类型 123456money = 50print(\"我还有\", money, \"块钱\")print(type(money))type_int = type(money)print(type_int)print(type(type_int)) 输出结果为 1234我还有 50 块钱&lt;class 'int'&gt;&lt;class 'int'&gt;&lt;class 'type'&gt; 2.4 数据类型显式转换 可以使用 int(x) float(x) str(x) 转换成相应的数据类型 注意 任何类型都可以转换成字符串类型，但是转换为int和float类型之前需要确保能转换到相应的类型 2.5 运算符 常用运算符 + - * / % // 两个斜杠为取整除,即只保留结果的整数部分 ** 两个星号为幂运算 1234print(9//2)print(9.0//2.0)print(3**2)# 输出结果为4 4.0 9 not：取反运算符 and：与运算符 or 或运算符 成员运算符 - in：在指定序列中找到指定的值则返回True，否则返回False - not in 身份运算符 - is：判断两个标识符是否引用了同一个对象 - not is 2.6 字符串 字符串定义法：单引号，双引号，三引号（可以换行） 字符串内部可以使用转义字符 \\ 来输出引号 可以使用加号（+）来对字符串进行拼接，但是不用拼接其他类型的变量,需要转换成字符串类型 ### 字符串格式化表示法 形式类似于c语言中printf括号中的内容 123massage = \"第%d个苹果重%.2f%s\" % (3, 1.2, \"kg\")print(massage)# 输出结果：第3个苹果重1.20kg 此处类似于%5.4f格式的效果与C语言相同 注意，在本格式化表示方法中，小数会自动在后面添加若干个0 字符串快速格式化 f\"内容{变量}\"的格式来快速格式化 123money = 1.2massage = f\"我有{money}这么多钱\"# 输出结果：我有1.2这么多钱 此种方法不需要进行精度处理，数字内容不变，小数位数也不变 2.7 input语句 使用 input() 语句可以从键盘获取输入 使用一个变量接收（存储）input语句获取的键盘输入数据即可 无论输入的数据是什么，都是存储为string类型的 第三章 Python判断语句 3.1 布尔类型和比较运算符 bool类型表示真和假 True 为真，为 1 False 为假，为 0 比较运算符与C语言相同，注意，==表示判断是否相等 3.2 if语句 语法格式： 123456789if 条件： 语句1elif 条件2： 语句2elif 条件3： 语句3...else： 语句N 注意，一定要进行缩进，python通过缩进的对齐判断语句的归属 条件判断语句的结果一定是bool类型的结果 第四章 Python循环语句 4.1 while循环 语法格式： 1234while 条件： 语句1 语句2 ... ## 4.2 for循环 - while循环的循环条件是自定义的，自行控制循环条件 - for循环是一种”轮询”机制，是对一批内容进行”逐个处理”将数据集中的每一个内容取出来进行处理。for循环是无法定义循环条件的，只能从数据集中依次取出内容 语法格式： 12for 临时变量 in 待处理数据集: 循环满足条件时执行的代码 例如： 12345name = \"itheima\"# for循环处理字符串for x in name: print(x)# 运行结果为将字符串中字符一个一个拿出来输出 语法格式中\"待处理数据集\"实际上为一个序列类型，其中包括字符串，列表，元组等等 通过range语句可以获得一个简单的数字序列，用于for循环中 range(num) 为从0到num的数字序列(不包含num本身) range(num1,num2) 为从num1到num2的数字序列(不包含num2本身) range(num1,num2,step) 为从num1到num2的数字序列(不包含num2本身)，步长为step 临时变量在编程规范上，作用范围（作用域）只限定在for循环内部 如果在for循环外部访问临时变量： 实际上是可以访问到的。但是在编程规范上，是不允许、不建议这么做的 4.3 continue和break 在嵌套循环中，只能作用在所在的循环上，无法对上层循环起作用 4.4 pass pass是空语句，不执行任何操作，一般用作占位语句，保持程序的完整性 第五章 Python函数基础 5.1 函数的基本定义 123def 函数名(传入参数)： 函数体 return 返回值 应先定义函数，后调用函数 参数不需要，可以省略 返回值不需要，可以省略 可以在def一行下面采用多行注释的方式写该函数的说明文档 5.2 None类型 Python中有一个特殊的字面量：None，其类型是：&lt;class 'NoneType'&gt; 无返回值的函数，实际上就是返回了：None这个字面量 None表示：空的、无实际意义的意思 return None，效果等同于不写return语句 在if判断中，None等同于False 一般用于在函数中主动返回None，配合if判断做相关处理 用于声明无内容的变量上 定义变量，但暂时不需要变量有具体值，可以用None来代替，例如name = None 5.3 局部变量与全局变量 在某一函数体内的为局部变量，只能在该函数体内生效 使用 global 关键字可以在函数内部声明全局变量 第六章 Python数据容器 6.1 list 列表 列表的特点： - 可以容纳多个元素（上限为2**63-1个） - 可以容纳不同类型的元素（混装） - 数据是有序存储的（有下标序号） - 允许重复数据存在 - 可以修改（增加或删除元素等） 6.1.1 列表的定义 12345# 定义变量变量名称 = [元素1, 元素2, ...]#定义空列表变量名称 = []变量名称 = list() 以 [] 作为表示，列表内每个元素之间以 , 隔开 列表可以一次存储多个数据，并且可以是不同的数据类型，支持嵌套 6.1.2 列表的下标索引 从头开始第一个的索引为0，一直到最后，如 list[2] 从尾开始最后一个的索引是-1，一直到最前 如果是列表嵌套列表，则可以使用类似C语言二位数组的表达方式，如 list[1][2] 要注意下标索引的取值范围，超出范围无法取出元素，并且会报错 6.1.3 列表的常用方法 在Python中，如果将函数定义为class（类）的成员，那么函数会称之为：方法 方法的使用格式为 对象.方法(传入参数) 方法 作用 列表.index(元素) 查找指定元素在列表的下标找不到报错ValueError 列表[下标] = 值 修改特定位置的值 列表.insert(下标, 元素) 在指定的下标位置，插入指定的元素 列表.append(元素) 将指定元素，追加到列表的尾部 列表.extend(其它数据容器) 将其它数据容器的内容取出，依次追加到列表尾部 del 列表[下标] 删除列表指定下标元素 列表.pop(下标) 删除列表指定下标元素 列表.remove(元素) 从前向后，删除此元素第一个匹配项 列表.clear() 清空列表 列表.count(元素) 统计此元素在列表中出现的次数找不到报错ValueError len(列表) 统计容器内有多少元素 6.2 tuple 元组 元组的特点： - 有序、任意数量元素、允许重复元素 - 不可修改 元组内容不可以直接修改，会报错 如果元组内部有列表元素，列表中的内容可以修改 6.2.1 列表的定义 12345# 定义元组变量名称 = (元素1, 元素2, ...)#定义空元组变量名称 = ()变量名称 = tuple() 如果定义的元组只有一个元素时，在元素1后面要加一个逗号，例如 (1,)，不然则不是元组类型 6.2.2 元组的下标索引 写法与列表相同，索引从0开始计算 6.2.3 元组的常用方法 元组.index() 元组.count() len(元组) 实际效果见6.1.3表格 6.3 str 字符串 字符串的特点： - 只可以储存字符串，长度任意 - 不可修改，如果要修改，只能得到一个新的字符串 6.3.1 字符串的下标索引 类似于C语言中字符数组，可以通过下标索引取出字符，索引从0号开始计算，但是没有最后的‘\\0’占位符 6.3.2 字符串的常用方法 str.index(参数)参数可以是一个字符，也可以是一个字符串 str.replace(m,n)将字符串中的m字符串替换为n字符串，返回一个新的字符串 str.split(分隔符)按照指定的分割符将字符串分为多个字符串，返回一个列表对象 str.strip(参数)去除字符串前后的参数中的内容，注意是按照字符去除，而不是整体的字符串匹配。如果不传入参数则默认去除字符串前后的空格和换行 str.count(参数) len(str) 6.4 序列的切片 序列是指能用下标索引的容器，例如：列表、元组、字符串 切片：从一个序列中取出一个子序列 语法：序列[起始下标：结束下标：步长]，返回一个新的序列 结束下标表示到某位结束，不包含结束位 起始下标留空表示从头开始，结束下标留空表示截取到结尾 步长可以为正数，也可以为负数，表示反向取，主要这时起始下标和结束下标也要反向标记 str[::-1]相当于将字符串从尾到头倒序输出 6.5 set 集合 集合的特点： - 无序，不支持下标索引访问 - 可以修改 - 数据不能重复 - 可以容纳多个数据，不同的数据类型 6.5.1 集合的定义 1234# 定义集合变量名称 = {元素1, 元素2, ...}#定义空集合变量名称 = set() 6.5.2 集合的常用方法 set.add(ele) set.remove(ele) set.pop()没有参数，随机取出一个元素，并将该元素从集合中去除 set.clear()清空集合 set1.difference(set2)取出集合1对集合2的差集（集合1有集合2没有的内容），返回一个集合 set1.difference_update(set2)对比集合1和集合2，在集合1内删除和集合2相同的元素。集合1被修改，集合2不变 set1.union(set2)将两个集合合并，返回一个新的集合 len(set) 6.6 dict 字典 6.6.1 字典的定义 12345# 定义字典变量名称 = {key: value, key: value, ...}#定义空字典变量名称 = {}变量名称 = dict() 6.6.2 字典的常用方法 访问时可以使用dict[key]获取某一键的值 dict.keys()返回包含所有键的列表 dict.has_key(k)检查字典中是否含有某一键 dict.values()返回包含所有值的列表 dict.get(key)返回字典的键 dict.items()返回有键值对组成的元组 dict.copy()复制字典到另一字典 del dict(key)删除指定键的元素 dict.clear()清除键的所有元素 dict.pop(key)弹出某一键的值，并从字典中删除 第七章 Python函数进阶 7.1 函数的多返回值 return函数后面可以写多个返回值，用逗号隔开 1234def test_return(): return 1, \"hello\", Truex, y, z = test_return()# x,y,z会对应接收到相应的值 7.2 函数的多种传参方式 Python的传参是传对象引用，是一个对象的内存地址，是传值和传址的一个综合 如果收到的是一个可以改变（字典或者列表）的对象的引用，就能修改对象的初始值；如果是一个不可修改（数字、字符、字符串或者元组）的对象，则不能直接修改原始对象。 7.2.1 位置参数 调用函数时根据函数定义的参数位置来传递参数 7.2.2 关键值参数 函数调用时通过“键=值”形式传递参数 123456def user_info(name, age, gender): print(f\"姓名是:{name}, 年龄是:{age}, 性别是:{gender}\")# 关键字参数user_info(name='小王', age=11, gender='女')user_info(age=10, gender='女', name='潇潇') # 可以不按照参数的定义顺序传参user_info('甜甜', gender='女', age=9) # 不表明名称的位置需对应 7.2.3 缺省参数 参数提供默认值，调用函数时可不传该默认参数的值 注意：所有位置参数必须出现在默认参数前，包括函数定义和调用 12345def user_info(name, age, gender='男'): print(f\"姓名是:{name}, 年龄是:{age}, 性别是:{gender}\")user_info('小天', 13, '女')user_info('小乐', 13) 7.2.4 不定长参数 不定长参数有两种类型 1. 位置传递（元组，*号） 2. 关键字传递（字典，**号） 1234567891011# 不定长 - 位置不定长, *号# 不定长定义的形式参数会作为元组存在，接收不定长数量的参数传入def user_info(*args): print(f\"args参数的类型是：{type(args)}，内容是:{args}\")user_info(1, 2, 3, '小明', '男孩')# 不定长 - 关键字不定长, **号def user_info(**kwargs): print(f\"args参数的类型是：{type(kwargs)}，内容是:{kwargs}\")user_info(name='小王', age=11, gender='男孩') 7.3 函数可以作为参数传递 函数本身是可以作为参数，传入另一个函数中进行使用的。 将函数传入的作用在于：传入计算逻辑，而非传入数据。 1234567891011# 定义一个函数，接收另一个函数作为传入参数def test_func(compute): result = compute(1, 2) # 确定compute是函数 print(f\"compute参数的类型是:{type(compute)}\") print(f\"计算结果：{result}\")# 定义一个函数，准备作为参数传入另一个函数def compute(x, y): return x + y# 调用，并传入函数test_func(compute) 在如上的例子中，test_func函数中需要一个函数，对1和2进行某种运算，外部定义了一个compute作为参数传递给了test_func函数 7.4 lambda匿名函数 lambda关键字，可以定义匿名函数（无名称），无名称的匿名函数，只可临时使用一次。 lambda函数基本语法：lambda 传入参数：函数体（一行代码） 第八章 文件操作 8.1 打开文件 使用open函数打开文件，并返回一个文件对象 myfile = open(file_name [, mode='r' [ , buffering=-1 [ , encoding = None ]]]) mode为文件的读写模式 | mode | 模式 | 文件不存在时 | 是否覆盖写 | | :---: | :---: | :----------: | :---------------: | | r | 只读 | 报错 | - | | r+ | 读写 | 报错 | 是 | | w | 只写 | 新建文件 | 是 | | w+ | 读写 | 新建文件 | 是 | | a | 只写 | 新建文件 | 否，从EOF处开始写 | | a+ | 读写 | 新建文件 | 否，从EOF处开始写 | 在字母后面加上b表示用二进制打开相应的文件，如rb+ 用w或a打开文件时，文件指针指向文件末尾；用r打卡文件时，文件指针指向文件开头 encoding为编码格式（推荐使用UTF-8）。encoding的位置不是第三位，如果要说明的话，应使用关键字参数。也可省略。 8.2 文件的常用属性 file.name：返回文件的名称； file.mode：返回打开文件时，采用的文件打开模式； file.encoding：返回打开文件时使用的编码格式； file.closed：判断文件是否己经关闭。 8.3 文件的常用方法 file.close() file.flush()刷新输出缓存 file.read([size])如果设置了size，则读取size字节；如果没设置，则读取问价全部内容。 file.readline([size])如果设置了size，size小于改行总字节，则读取size字节；如果大于，则读取该行的所有内容 file.readlines()读取完文件，返回每行组成的列表 file.write(str)将字符串写入文件 file.writelines(sequence of strings)写多行到文件，参数可以是字符串，也可以是列表 file.seek(n)将文件指针移动到第n字节，0表示开头 file.tell()返回文件指针当前的位置 8.4 shutil模块 shutil模块中包含一些函数，用于复制、移动、改名和删除文件 第九章 Python异常、模块与包 9.1 异常 捕获常规异常 12345678try: 可能发生错误的代码except: 如果出现异常执行的代码else: 如果异常不出现执行的代码finally: 无论异常是否出现都要执行的代码 捕获指定异常 12345try: print(name)except NameError as e: print('name变量名称未定义错误')# 如果尝试执行的代码的异常类型和要捕获的异常类型不一致，则无法捕获异常。 捕获多个异常，可以把要捕获的异常类型的名字，放到except 后，并使用元组的方式进行书写 1234try: print(1/0)except (NameError, ZeroDivisionError): print('ZeroDivision错误...') 捕获所有异常，并输出 1234try: print(name)except Exception as e: print(e) 异常是具有传递性的 当我们想要保证程序不会因为异常崩溃的时候, 就可以在main函数中设置异常捕获, 由于无论在整个程序哪里发生异常, 最终都会传递到main函数中, 这样就可以确保所有的异常都会被捕获 9.2 模块 模块是一个python文件，里面有类、函数、变量等等，可以导入模块来用 模块的导入语法： [from 模块名] import [模块|类|变量|函数| * ] [as 别名] 模块的使用： 模块名.功能名 12345678910# 使用import导入time模块中的sleep功能# 方法1import timetime.sleep(5)# 方法2from time import sleepsleep(5)# 方法3from time import *sleep(5) 注意：当导入多个模块，且模块内有重名的功能时，调用这个重名的功能时，使用的是后导入模块的该功能 if __name__ == '__main__': 可以用于某模块内部的运行代码，防止在导入模块时执行内置语句 __all__ = [' ' , ' ' , ...] 当模块文件中有 __all__ 变量，使用 from xxx import * 导入时，只能导入列表中的元素 9.3 包 包就是一个文件夹，其中包含了很多的py文件和一个_init.py文件","link":"/2022/08/30/python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/"},{"title":"密码学期末复习（PPT）","text":"1. 概述 1.1 引言 1949年，香农的《保密系统通信理论》将密码学推向基于信息论的学科，近代密码学诞生。 1976年，Diffie和Hellman提出公钥密码学的思想，开启了现代密码学。 我国密码分级：核心密码（核心机密）、普通密码（高于商用）、商用密码（非机密）、个人密码（个人隐私）。前三种密码都由国家密码管理局统一管理。 1.2 网络安全概念 信息安全三要素（CIA）： 机密性 数据保密性：隐私或者秘密数据不向非授权者泄露、也不被他们使用 隐私性：个人能够控制和确定自身相关的哪些信息可以被收集、保存、公开及向谁公开 完整性 数据完整性：信息和程序只能以特定的方式进行改变 系统完整性：系统以一种正常的方式执行预定的功能，免于被非法的操控 可用性 除此之外，还需要一些安全概念： 真实性：一个实体是真实、可被验证的，或者信息和信息的来源是正确的。 可追溯性：实体的行为可以追溯到唯一的该实体。 安全泄露事件的影响： 执行使命的能力 资产损失 经济损失 对个人的伤害 低 能力一定程度降低，效果稍有降低 较少 很小 很小 中 能力显著降级，能完成主要功能，但效果明显降低 显著 显著 显著，但是不威胁生命安全 高 能力严重降级，不能完成一项或多项功能 大部分 大部分 严重，包括威胁生命安全 OSI安全框架：安全方面：攻击、机制和服务 安全攻击：任何危及信息系统安全的行为。 安全机制：用来检测、阻止攻击或从攻击状态恢复到正常状态的过程。 安全服务：利用一种或多种安全机制进行反攻击。 安全攻击 安全漏洞是信息系统产生安全问题的内因，也叫缺陷、隐患、脆弱性。 安全威胁是信息系统产生安全问题的外因，也叫攻击。 安全威胁分为： 自然威胁：各种自然灾害、设备老化等 人为威胁：对信息及信息系统的人为攻击，通过找弱点，到达破坏、欺骗等效果 攻击方式有两种：主动攻击和被动攻击。 被动攻击：窃听，不影响正常的通信，不对信息进行任何修改，以获取信息为目的。 被动攻击分为：信息内容的泄露、流量分析。 主动攻击：对数据流进行篡改，或者产生假的信息。 主动攻击包括： 拒绝服务：对系统可用性进行攻击。 消息修改：修改消息内容、延迟传输、修改消息顺序等。破坏完整性。 伪装：假装成其他实体。对真实性进行攻击。 重放：将截获的信息再次发送。 安全服务：对系统资源进行特殊保护的处理或者通信服务。 数据保密性：防止消息内容泄露，被窃听。防止被动攻击 认证：保证通信的真实性。包括单向通信和双向通信 数据完整性：保证所接受的信息是未经修改或重放的，还能用于对一定程度损坏的数据的恢复。 不可否认性：接收者能证明消息的真实来源，发送者能证实接收者已经接受了消息 访问控制：检查用户是否对某一资源有访问权。实现方式是认证 安全机制 大多数机制的共同点：密码技术 特定的安全机制：加密、数字签名、访问控制、数据完整性、认证交换、流量填充、路由控制、公证 普遍的安全机制：可信功能、安全标签、事件检测、安全审计追踪、安全恢复 安全服务与机制间的联系（表格） 网络安全模型：两个基本成分和一个可选成分： 消息的安全变换 通信双方共享的秘密信息 有时需要一个可信第三方 3. 传统加密技术 3.1 密码学发展史 三个阶段： -1949：古典密码 1949-1975：计算机使得基于复杂计算的密码成为可能。数据的安全性基于密钥而不是算法的保密。 1976-：公钥密码、对称密码进一步发展 3.2 对称密码模型 密钥为，密钥可能值的范围为密钥空间。加解密函数为： 对于加解密过程，明文加密后再解密得到的内容必须和原来明文相同。 密码学：研究信息的保密和复原保密信息以获取其真实内容的学科称为密码学。它包括密码编码学和密码分析学。 密码编码学：研究对信息进行编码实现隐蔽信息的一门学科。 密码分析学：不知道任何加密细节的条件下解密消息的技术，即“破译”。 对称密码： 又称传统密码、常规密码、私钥密码、单钥密码 发送方和接收方共享一个共同的密钥。 对称密码安全的两个必备条件： 加密算法必须足够强 发送者和接收者在某种安全的形式下获得密钥并且保证密钥的安全 Kerckhoff原则：系统的保密性不依赖于对加密体制或算法的保密，而依赖于对密钥的保密。 密码编码学的三个特征： 明文转换为密文的运算类型：代替（元素映射到另一个元素）、置换（元素的重新排列）。要求运算是可逆的。 所用的密钥数：单钥密码（基于计算安全性，即破译的计算量下限）、双钥密码（基于可证明安全性，即依赖数学难题） 处理明文的方法：分组密码、序列密码（流密码） 密码分析学： 目标：得到密钥 攻击方法：密码分析（利用算法的性质、明文的特性、明密文对）、穷举攻击 基于密码分析的攻击： 唯密文攻击 已知明文攻击：已知多个明文-密文对 选择明文攻击：由破译者选择的明文信息及其密文 选择密文攻击：由破译者选择的密文信息及其明文 选择文本攻击：选择明文+密文攻击 无条件安全：算法产生的密文不能给出唯一决定明文的足够信息，此时敌手获得多少密文用多长时间都不能解密密文。仅当密钥至少和明文一样长时，才能达到无条件安全。即只有一次一密方案是无条件安全的。 计算上安全：破译密文的代价超过被加密信息的价值或破译密文所花的时间超过信息的有用期。计算上安全弱于无条件安全。 穷举攻击：敌手想找出一个私钥，采用穷举攻击。若k是均匀随机分布的，则敌手需要平均尝试： 3.3 代替技术 凯撒密码 字母表位移k位 共有26种可能的密钥，25种有效 加密： 解密： 采样密码（乘法密码） 明文字母表每隔k位取出一个 要求密钥k与26互素，有 个 可用密钥 加密： 解密： 仿射代替密码 加法和乘法结合 q=26时，共26*12-1 = 311 个可用密码（减去的是不变的变换） 加密： 解密： 单表代替密码： 随机映射 密钥数目： 缺陷：密文字母带有明文的统计特性，明文中一个元素仅影响密文中一个元素 语言的统计特性： 单字母：极大概率：e，大概率字母：tao，较大概率：inshr 多字母 单词特性 单表代替密码之所以容易被攻击，是因为每个密文字母都是用同一个代替密表加密而成的，相同的密文字母对应着相同的明文字母。实际上只是改变字母的名称。 Playfair密码 多字母代替密码，5*5矩阵，先填充单词，再填充剩下的。I=J 加密：明文两个字母一组，如果两个字母相同则在中间插入一个填充字母。同行：循环向右，同列：循环向下，对角线：另一对角线，按加密两字母的行排序 优点：安全性优于单表代替密码，使用频率分析困难 Hill密码 多字母代替密码，利用矩阵 加密： 解密： 优点：完全隐藏了单字母的频率特性，可以抵抗唯密文攻击 容易被已知明文攻击破解 3.4 多表代替技术 多表代替：对每位明文采用不同的单表代替。当明文和密钥一样长时成为一次一密密码。 维吉尼亚密码：明文，密钥，密文 26*26表格，行和列均为a-z，表内为每行以a.b.c...z开头的凯撒密码 加密： 解密： 博福特密码： 加密： 解密： 弗纳姆Vernam密码： 与密钥诸位异或 一次一密： 在Vernam密码中，如果采用的密钥不重复就是一次一密体制 在理论上不可被破解，但是在实际上不可行 多表代替密码的破译： 对于周期多表代替密码，可以转换成多组单表代替密码进行破译。 3.5 其他技术 置换技术 轮转机 隐写术 4. 分组密码和数据加密标准 4.1 Feistel密码 分组密码：明文分组加密，得到等长的密文分组。解密算法是加密算法的逆运算。 可逆变换：每个明文分组唯一对应一个密文分组。映射的总数是 。称为理想分组密码。 混淆：是密文和加密密钥之间关系变得复杂，以阻止攻击者发现密钥。 扩散：每个密文字母尽可能受多个明文数字的影响，使得明文的统计特性消散在密文中。 乘积密码：在单个加密机制中依次使用两个或两个以上不同类型的基本密码，所得结果的密码强度将强于每个单个密码的强度。 Feistel密码是一种特殊的SP网络，交替使用代替和置换，增强密码的扩散和混淆性能。 Feistel密码的结构：加密和解密完全相同 在Feistal密码结构中，中间经过若干轮上述基本结构，加密和解密的最后都要添加一步交换L和R F函数不必可逆。 Feistel密码的解密过程与加密过程实质相同，无需区分实现加密和解密算法，只是密钥使用顺序不同。 Feistel密码的设计： 分组越大，安全性越高，但是计算越慢 密钥越长，安全性越高，但是计算越慢 循环越多，安全性越高 子密钥产生算法和轮函数设计越复杂，安全性越高 DES、SM4为分组密码 3.2 数据加密标准(DES) DES（Data Encryption Standard）使用56比特的密钥加密64位的明文，得到64位的密文。 算法的实现过程： 初始置换IP： 16轮迭代运算：，子密钥长48位 迭代运算过程包含4步： --32bit--&gt; 扩展置换E --48bit--&gt; 与子密钥异或 --48bit--&gt; S盒代替 --32bit--&gt; 置换运算P --32bit--&gt; 逆置换IP-1： S盒：唯一的非线性变换，决定了算法的安全强度，起到混淆的作用。DES中的S盒接受6bit，第一位和最后一位组成选择行，中间4位选择列，输出4比特 子密钥的产生： 密钥扩展算法要求子密钥的统计独立性和灵敏性，从一些子密钥中获得其他子密钥在计算上是困难的。 雪崩效应：明文或密钥的一点小的变动都引起密文的较大变化。P置换的目的是提供雪崩效应。 弱密钥：初始密钥产生的16个子密钥相同。 半弱密钥：存在不相等的，使得。DES一共有6对半弱密钥。 差分密码攻击：通过比较两个已知明文差异的明密文对，在使用相同子密钥的情况下搜索密文中的已知差异。 线性密码分析：基于找到DES中进行变换的线性近似，可以在有2^47个已知明文的情况下破译DES密钥，但实践中仍不可行。 分组密码的整体结构：Feistel结构和SP网络 SP网络结构：每一轮中，轮输入首先被一个由子密钥控制的可逆函数S（混淆）作用，然后再对所得结果用置换（或可逆线性变换）P（扩散）作用。SP网络结构可以更快的扩散，但加解密通常不相似。 3.3 多重加密与三重DES算法 闭合的加密算法：对于任意密钥，都存在，使得： 对于闭合的加密算法，多次加密并不能增强安全性。但是DES不是一个闭合的加密算法，所以二重DES和三重DES有一定的价值。 二重DES的强度不等于56*2=112bit密钥的密码强度，会受到中途相遇攻击： 中途相遇攻击： 加密： 已知一对。对所有可能的种对进行加密，得到z，存储为一个字典。对所有可能的种对进行解密，得到z，此时可以查找上述字典，配对x和y，一旦找到则确定两个密钥。 两重DES的密文有种，密钥有种，因此有种密钥能产生给定的密文。 攻击法最大的实验次数是次，即算法难度为。 如果再用一对进行对密钥的验证，则错误率降为。 三重DES： 加密： 解密： 3.4 DES的差分密码分析 通过分析特定明文差分对相对应密文差分影响来获得尽可能大的密钥。 6. 高级加密标准 6.1 代换-置换网络(SPN) 乘积密码体制：先用一种密码进行加密，对加密结果使用另一种方法进行加密。如果第二种加密方式是自己，则为二重成绩，记为。n重成绩记为。 如果，则称密码是幂等的。幂等密码体制和自己做乘积，不能提高算法安全性。古典密码大部分都是幂等的。 迭代密码：如果密码不是幂等的，则多次迭代有可能提高安全性。一种构造简单的非幂等密码体制的方法是对两个不同的密码体制做乘积，并且保证两个密码体制是不能交换的。 在代换(S)-置换(P)网络中：加密：，解密： 对于任意的线性变换，都有： 6.2 高级加密标准(AES) Rijndael是一个迭代型分组密码，其分组长度和密钥长度都可变，可以为128比特、192比特、256比特。Rijndael使用非线性结构的S-boxes，能抵抗所有已知的攻击。Rijndael中轮函数由3层不同的可逆均匀变换组成，分别为线性混合层、非线性层、密钥加层。 在第一轮之前使用一个初始密钥加层，目的是：使加解密相似、对安全性无意义、有利于差分分析。 AES是一个迭代型分组密码，其分组长度固定为128 比特，密钥长度则可以是128，192或256比特 AES流程： 128、192、256比特分别为：16、24、32字节，4、6、8字（），迭代轮数分别为10、12、14（） 分组是以字节为单位的4*4方阵.按列排序： 1234a0 a4 a8 a12a1 a5 a9 a13a2 a6 a10 a14a3 a7 a11 a15 字节代替：S盒，非线性变换，输入8位，高4位为行值，低四位为列值，输出8位 行移位：第0行不动，第1.2.3行循环左移1.2.3位。经过行移位后，1列中4个元素被分布到不同的列中。 列混淆：矩阵乘法，运算范围是：。 列混淆的逆变换： 列变换也可以通过多项式计算来定义：把状态矩阵的列视为多项式， 其中要求与模数多项式互素，这样才存在逆多项式进行逆运算。 列混淆的原理：矩阵系数是码字间最大距离的编码（MDS码），这使得有良好的混淆性。经过几轮的列混淆和行移位后，所有的输出位与所有的输入位都有关。 密钥轮加：与密钥进行逐比特异或。 密钥扩展：共生成字，前4字用于和明文异或，后面每4字用于每轮的密钥轮加中。 扩展过程中，如果下标不是4的倍数，则；如果是4的倍数，则将向左循环移动一个字节，并进行字节代换，与轮常数Rcon异或。 使用轮常数消除对称性；密码密钥的差异扩散到轮密钥中的能力，即密钥的每个位能影响到轮密钥的许多位；已知部分密码密钥或部分轮密钥比特, 不能计算出许多其它轮密钥比特；足够的非线性性, 防止只从密码密钥的差分就能完全决定所有的轮密钥差分。 AES的加密和解密的轮结构顺序不同，但是逆向行移位与逆向字节代替、轮密钥加和逆向列混淆可以交换。 行移位改变顺序，字节代替改变内容，互不影响；可以将轮密钥做逆向列变换后，则可以先逆向列混淆，在与变换后的轮密钥异或。 AES的安全性：可以抵抗差分攻击和线性密码攻击，目前还不存在快于穷举攻击的攻击方式。 评价：运算均在2^8有限域上，除了查表操作外均为简单的异或和移位操作。每一轮密钥扩展中都有非线性变换，增强了密码的抗攻击能力。 7. 分组加密的工作模式 7.1 电话本模式 ECB 明文分组，每组都使用相同的密钥加密。同一明文组总产生同样的密文组。 如果最后一组不足64位，则需要填充。 适合短消息加密，应用长消息时容易受到攻击。 7.2 密文分组链接模式 CBC 加密算法的输入是当前明文分组和前一次密文分组的异或，每个密文块依赖于前面的所有明文块。 需要使用初始化向量，IV应像密钥一样被保护。需要填充。 加密时无法并行处理。解密时，从两个邻接的密文块即可得到一个明文块，因此解密过程可以被并行化。 如果传输时发生错误，则恢复的明文中只有这个块和下一块的内容发生改变。 解密时，密文中1位的变化只会改变这个块和下一块的内容改变。 加密过程为： 7.3 密文反馈模式 CFB 利用CFB（cipher feedback）模式或OFB模式可将分组对称密码转换为流密码。 不需要对消息填充，而且运行是实时的。 需要初始向量。 加密过程不能并行化，解密过程可以。 对信道错误较敏感，错误传播：1个比特密文传输错误会传播约64/j个分组。 加密过程： 、 7.4 输出反馈模式 OFB 与CFB的区别是加密算法的输出反馈到下一轮。 优点：传输过程中的比特错误不会被传播；缺点：比CFB模式更易受到对消息流的篡改攻击。 加密过程： 7.5 计数器模式 CTR CTR将块密码变为流密码。它通过递增一个加密计数器以产生连续的密钥流，其中，计数器可以是任意保证不产生长时间重复输出的函数，使用一个普通的计数器是最简单和最常见的做法。 不需要填充 可并行加密，可预处理（明文不参与密钥生成），加密数据块随机访问（只需要加计数器） 加密过程： 8. 伪随机数的产生和流密码 8.1 伪随机数的产生原则 随机数序列需要满足两个特征：随机性和不可预测性 随机性： 分布均匀性：0和1均匀分布 独立性：任何子序列不能由其他子序列推导出 序列是否满足分布均匀性可以检测得出，但是是否满足独立性无法检测。有一些算法可以检测出不满足独立性。 不可预测性： 序列以后的数是不可预测的。 真随机数列是不可预测的，因为各个位之间互相独立。 伪随机数序列要注意不可预测性。 真随机数：物理噪声、高质量随机数编辑成书、真随机数发生器（TRNG） 伪随机数：使用算法生成随机数，由伪随机数发生器（PRNGs）产生 伪随机数发生器（PRNG）：生产不限长度的位流，通常作为对称流密码的输入。 伪随机函数（PRF）：用于产生固定长度的伪随机数串。如对称加密的时变值。PRF的输出常为种子加一些上下文相关的特定值。 对PRNG的要求： 通用要求：输出保密性：不知道种子的敌手不能知道伪随机串。 特定要求：随机性、不可预测性、种子的特性 随机性：尽管生成的位流是确定的，但是要显示是随机的。 不可预测性：前向不可预测性、后向不可预测性 种子的特性：安全、不可预测 PRNG的设计 特意设计的PRNG算法 基于现存密码的算法，如对称分组密码、非对称密码、Hash函数等 8.2 伪随机数发生器 8.2.1 线性同余发生器 参数：m模，a乘数，c增量，X0初始值或种子 迭代算法： 评价线性同余发生器的性能： 迭代函数应该是全周期的，级重复0 - m-1之间的所有书 产生的序列应该看上去是随机的 能有效地利用32位运算器方便的实现 在计算机中，为了满足上述指标并便于运算，m一般取，c=0，a是m的一个本原根（如7^5） 线性同余算法只在初值X0的选取具有随机性，算法本身无随机性，以后的数被确定性的产生了。 如果敌手知道算法的参数，只要知道当前的一个数，就知道了后续的所有数。 敌手知道数列中的极少一部分，就可以确定出算法的参数。 可以利用内部系统时钟修正随机数数列：产生几个数后用时钟值作为种子；或者将当前时钟值加到每个随机数上。 8.2.2 BBS发生器 参数：两个大素数，，一个随机数s与n互素 算法： 1234X[0] = s ** 2 % nfor i in range(length): X[i] = X[i-1] ** 2 % n B[i] = X[i] % 2 BBS的安全性基于大整数分解的困难性，是密码安全伪随机数比特产生器 密码安全伪随机数比特产生器：以伪随机比特产生器的输出序列的前k个比特作为输入，不存在多项式时间算法，能以大于1/2的概率预测第k+1个比特。 8.3 使用分组密码的伪随机数发生器 使用分组密码CTR模式和OFB模式： ANSI X9.17：使用3个三重DES加密，输出一个64比特的伪随机数和一个64比特的新种子 8.4 流密码 伪随机数发生器的输出称为密钥流，密钥流和明文流的每一个字节进行按位异或运算，得到一个密文字节。密码系统的安全性取决于密钥流的性能。 流密码类似于一次一密，但是一次一密使用的是真正的随机数流。 一次一密密码：密钥流是完全随机序列，且密钥不重用。一次一密在唯密文攻击条件下是理论保密的，这是唯一一个能被证明是无条件安全的密码。 流密码可以分为：同步流密码和自同步流密码 同步流密码：密钥流生成器独立于明文字符。 对于明文而言，加密过程是无记忆的；解密过程要求密钥流和加密密钥流完全同步。 通信双方有相同的种子序列和初始状态，就可以产生相同的密钥。 自同步流密码：密钥流生成器与明文字符有关。 是有记忆的：时刻的密文与时刻的明文和时刻之前个明文符号有关。 有自同步能力。 明文每个字符扩散在密文多个字符中，强化了抗统计分析的能力。 8.5 线性反馈移位寄存器 LFSR 密钥发生器的组成：驱动部分（提供统计特性好的序列）和非线性组合部分（变换成密码学特性好的序列） 反馈移位寄存器是序列密码设计中常用乱源。目的是以种子密钥为序列的初态，按照确定的递推关系，产生一个周期长、线性复杂度高、统计特性好的初始乱源，然后利用密码变换，最终产生抗破译能力强的乱数序列。 一个反馈寄存器由两部分组成：移位寄存器和反馈函数。如果反馈函数、是n个变量的线性函数，则称为线性反馈移位寄存器(LFSR)。输出的序列称为线性反馈移位寄存器序列，记为LFSR序列。 把多项式f(x)称为LFSR的特征多项式 例题： 反馈移位寄存器的功能完全由其反馈逻辑函数决定。 移位寄存器的序列存在周期。能达到最长周期的序列称为m序列。 表示方法： 8.6 流密码RC4算法 RC4分为密钥调度算法（KSA）和伪随机数生成算法（PRGA） ​ KSA使用密钥生成原始S表。 ​ PRGA使用S表产生流密钥序列。 加密单位是字节。密钥长度1-256字节任意。 密钥调度算法： RC4使用了一个2^8字节大小的非线性数据表(简称S表)，对S表进行非线性变换，得到密钥流。 12345S = [i for i in range(256)]T = [k[i % len(k)] for i in range(256)]for i in range(256): j = j + S[i] + T[i] % 256 S[i], S[j] = S[j], S[i] 伪随机数生成算法： 1234567i, j = 0, 0while True: i = i + 1 % 256 j = j + S[i] % 256 S[i], S[j] = S[j], S[i] t = S[j] + S[i] % 256 k = S[t] 通过设计合适的伪随机数发生器，当密钥长度相当时，流密码可提供和分组密码一样的安全性。 对于流密码，如果用流密码对两个明文加密且使用相同密钥，则密码分析就会相当容易：如果对两个密文流进行异或，那么得出的结果就是两个原始明文的异或。 9. 公钥密码与 RSA 9.1 公钥密码原理 公钥，私钥 可提供的安全服务： 保密性：任何人可以使用公钥加密，只有私钥持有者可以用私钥解密。 不可否认性：只有私钥持有者可以使用私钥签名，任何人可以使用公钥认证。 公钥密码技术研究的基本工具不再像对称密码技术那样是代替和置换，而是数学函数。公钥密码体制的这种安全性理论基础只是基于复杂性理论的一种计算安全性。 公钥密码体制由6部分组成：明文密文、公钥私钥、加密解密算法。 公钥密码体制的主要特点是采用两个密钥将加密和解密能力分开。 使用公钥密码进行加密时，由于任何人都可以使用公钥进行加密，因此得到的密文不具有认证性，即无法确定是谁发的。因此要同时实现保密性和认证性，要采用双重加密：先用自己的私钥进行签名，再用对方公开的公钥加密后公开。 公钥密码应该满足的条件： 单向函数不能用于加密。丹恒单向陷门函数对于不知道陷门的人表现出单向函数的特性，可以用于加密。因此设计公钥密码体制变成了寻找单向陷门函数。公钥密码思想的首创者Diffie和Hellmen指出：计算复杂性中的NP完全问题可被用作设计公钥密码算法。 可提供单向函数的数学难题是： 大整数分解问题：RSA 有限域的离散对数问题：DH协议 椭圆曲线上的离散对数问题：SM2 9.2 RSA公钥算法 RSA是一种分组加密算法，明文和密文在0到n-1之间，n是一个正整数，公钥为(e,n)，私钥为(d,n) 算法的数学基础是初等数论中的Euler(欧拉)定理，并建立在大整数因子分解的困难性之上 其中包括三个算法：密钥生成算法、加密算法、解密算法 密钥生成算法： 两个不同但是大小相近的大素数p和q，n=pq，整数e满足 ，计算d满足 。则 为公钥， 为私钥。 加密算法： 将明文M分组，使得每组的内容十进制数小于n，即分组长度小于（小于比n小的2的最大次幂）（分组大小，满足），对每组明文做运算： 解密算法： 算法的正确性证明 RSA的安全性是基于加密函数是一个陷门单向函数，陷门是分解，进而用欧式法则求出私钥d。 优点：第一个能同时用于加密和数字签名的算法，也易于理解和操作；符合计算机网络的环境；对于大量用户，可以将加密密钥用电话簿的方式印出。 缺点：产生密钥很麻烦；分组长度太大，运算代价很高，尤其是速度较慢。 9.3 RSA中的计算问题 快速模幂算法 平方和乘法将计算的模乘法数目缩小到至多，为c的二进制表示的比特数 如n以二进制表示有k比特，即，则有 因此计算可以在时间内完成。 为了提高加密速度，e可以取特定的小整数，如，或者3。但是RSA也是无法达到对称密钥的运行速度。具体应用中将RSA算法更多地用于密钥的安全传输过程，而在针对具体传输信息 的加密与解密中依靠对称密钥算法，这样便能够增进密码系统的运行速度。 解密的快速实现 密钥生成 采用概率素数判定测试(如：Miller Rabin)找到可使用的p和q 9.4 RSA的安全性分析 p和q大约是100位的十进制数，n长度不少于512比特 要很大，且pq的长度相同 p和q最好位强素数： 强素数需满足：p-1也有大的素数因子r，p+1也有大的素数因子，r-1仍有大的素数因子 9.4.1 因式分解攻击 有三种途径：分解n；能计算出；能直接确定d 转移到2048位的RSA、Diffie-Hellman或DSA密钥 9.4.2 参数选取不当造成的攻击 要很大 如果小，则也小，此时稍大于n，即稍大于 顺序检查大于的每一个数x，找到某个x使得可以被开方为y，则 9.4.3 选择密文攻击 RSA算法有同态的特点： RSA最优非对称加密填充（RSA-OAEP）可抗击适应性选择密文攻击。 9.4.4 共模攻击 不同用户之间不要共享整数n 若一个用户有一个模数n，而拥有多组不同的e和d。若存在同一信息P分别用不同的公钥加密，若e1和e2恰好互质（存在），则可以得到P： 9.4.5 小指数攻击 9.4.6 解密密钥的安全 在私钥d被泄露的情况下，整数n也不再安全，重新选择e也无法保证安全性，必须重新选择n。 计算解密密钥d的难度并不小于对整数n进行素因子分解的难度。 如果获得一个 的非平凡平方根，则可以在多项式时间内完成对n的分解。 10. 其他密码体制 10.1 Rabin密码体制 Rabin密码体制已被证明对该体制的破译与分解大整数是等价的 不以一一对应的单向陷门函数为基础，对同一密文，可能有两个以上对应的明文 密钥生成算法： 选择两个大素数pq满足，计算 n为公钥，p,q为私钥 加密： 解密：求解 ，由CRT知该方程等价于解方程组： 由于 ，方程组的解容易得出，每个方程都由两个解。因此可以得到四个可能的解，即每一密文对应的明文不惟一。 为了有效确定明文，可在m中加入某些双方商定的信息，如日期、发送者的ID等。 例题： 10.2 Diffie-Hellman密钥交换 Diffie-Hellman算法可以用于密钥分配，但是不能用于加密或解密信息。 安全性依赖于有限域上计算离散对数非常困难。 算法流程： AB协商一个大素数p和本原根a，a和p公开 A产生随机数x，计算，X发给B B产生随机数y，计算，Y发给A A计算 B计算 Diffie-Hellman密钥交换容易受到中间人攻击，原因是Diffie-Hellman密钥交换不认证对方。 改进的Diffie-Hellman密钥交换算法： 非交互双方密钥协商协议 有一组用户，每个用户都产生一个长期密钥Xi，并计算公开的Yi 全局g和a 任何用户都可以访问该用户的公开值，计算密钥，用密钥对消息加密后发送给A 优点：若该中心目录是可信的，则这种形式的通信既可保证保密性（只有i和j可以确定密钥，所有其他用户均不能读取该消息），又可保证某种程度的真实性（接收方i知道只有用户j能用该密钥产生消息）。 多方的Diffie-Hellman： 10.3 ElGamal密码体制 双钥密码体制，用于加密和签名 安全性基于求解离散对数问题的困难性。 密钥生成算法： 选取一个足够大的素数q，在上选取一个本原元a 随机选取一个整数 ，并计算 公钥为 ，私钥为 加密： 消息 。以分组密码序列的方式来发送信息，每个分组的信息大小不超过q。 选择任意整数 一次性密钥 得到密文对： 解密： 推导过程： 例题： 密文由明文和所选随机数来确定，因而是非确定性加密，或称为随机化加密。 如果信息必须分组然后以加密的密钥序列发送，那么每一个分块要有唯一的k。如果k用于多个分块。利用信息的分块M1，攻击者会计算出其他块。 方法：两个C2和做比，K被约掉。 10.4 ECC 10.4.1 椭圆曲线密码简介 基于椭圆曲线数学的一种公钥密码的方法。 椭圆曲线上的公钥密码体制的优点：速度快、密钥短、存储空间和传输带宽占用较少，安全性高，灵活性好。 基于椭圆曲线离散对数问题的困难性。 国家标准与技术局和ANSI X9已经设定了最小密钥长度的要求，RSA和DSA是1024位，ECC是160位，相应的对称分组密码的密钥长度是80位。 10.4.2 实域R上的椭圆曲线 无穷远点：平行线交于无穷远点，则平面上所有直线都有唯一的交点 椭圆曲线为曲线 上的点，外加无穷远点 其中参数ab满足 ，则椭圆曲线构成一个群，并定义加法运算 如果椭圆曲线上的三个点位于同一直线上，那么它们的和为无穷远点（零点）。 其中O为加法的单位元。，则负元定义为： 点Q的倍点：做Q的切线，与椭圆曲线交于点S， 10.4.3 Zp上的椭圆曲线 定义： 曲线所有的解(x,y)，连同无穷远点定义为Zp上的一个椭圆曲线，记为 上的椭圆曲线点加公式： 其中： 10.4.4 GF(2^m)上的椭圆曲线 方程为： 使用生成元 10.4.5 椭圆曲线密码体制 椭圆曲线密码体制(ECC)，其依据是定义在椭圆曲线点群上的离散对数问题的难解性。 ，且P的阶很大（，t很大）。取，则的求解是难处理的。 一般数域上的离散对数问题（以及大数分解问题）存在亚指数级时间复杂度求解算法，而ECDLP只有纯指数算法。 用椭圆曲线实现Diffie-Hellman密钥交换 用椭圆曲线实现ElGamal密码体制 例题： ECC技术要求： p越大越安全，但是会变慢，200bit左右即可 G是基点，n是G的阶，n应该为质数 h是椭圆曲线上所有点的个数m与n相除的商的整数部分， 优点; 安全性高 密钥量小 灵活性好 速度快 适合嵌入式设备 11. 密码学Hash函数 11.1 密码学hash函数的应用 哈希函数用于将任意长的消息映射为较短的、固定长度的一个值。对于大的输入集合使用该函数，输出结果应该分布均匀且看起来随机。 Hash函数首要目标是保证数据的完整性，对于任何一位或几位的改变都将极大可能改变其hash码。 散列函数是一种单向密码体制，即它从明文到密文是不可逆映射，并且找到两个不同的数据块对应相同的Hash值在计算上不可行。 MD系列：1978年，Merkle和Damagad设计MD迭代结构。 SHA系列：SHA1、SHA2都是迭代结构。Keccak被选为SHA-3，采用了创新的“海绵引擎”散列消息文本。 国密：SM3，采用MD结构，输出值为256比特。 哈希函数在密码学中的应用：消息认证码、谁在前面、伪随机数发生器、一次性口令 在区块链中的应用：快速验证、防止篡改、POW工作量证明 消息认证： 是验证消息完整性的一种服务。确保受到的数据不变且发送方的身份有效。 方法1：AB共享一个密钥，消息与其杂凑码链接后用单钥加密算法加密 方法2：用单钥加密算法仅对杂凑码加密，这种方式用于不要求保密性的情况下，可减少处理负担 方法3：AB共享一个秘密值S，A计算消息和秘密值链接在一起的杂凑值，并将此杂凑值附加到消息后发往B 方法4：在上一种方式的基础上，在消息与杂凑值链接以后再增加单钥加密运算 方法5：用公钥加密算法，将消息和用发送方的密钥签名消息的杂凑码连接在一起 方法6：方法5之后再使用单钥加密算法加密。这种方式提供了保密性和数字签名 数字签名： 使用用户的私钥加密消息的Hash值，其它任何知道该用户公钥的人通过数字签名验证消息的完整性。 可以减少签名长度，提高签名速度；可以不泄露签名对应的信息。 11.2 安全性需求 Hash函数安全性条件：伪随机性:映射分布均匀性和差分分布均匀性 使输入中每一个比特的信息，尽量均匀地反映到输出的每一个比特上去 输出中的每一个比特，都是输入中尽可能多比特的信息一起作用的结果 01个数大致相等、雪崩效应、1个变化一半以上变化 杂凑函数的攻击：伪造消息，使其与原来消息的杂凑码相同 攻击方法：穷举攻击（原像攻击和第二原像攻击、碰撞攻击）、算法分析 评价hash算法抗密码分析能力的方法是：与穷举攻击所需的代价相比，理想的hash函数算法要求密码分析攻击所需代价大于或等于穷举攻击所需代价。 穷举攻击：不依赖于任何算法细节，仅与Hash值长度有关，包括 原像攻击和第二原像攻击：给定hash，找到这个杂凑值的消息。攻击规模是，平均尝试次 碰撞攻击：找到两个信息满足哈希值相等。两种常用的攻击方法：生日攻击法和中点交会攻击法。 第一类生日攻击： 问题：H有n个可能的输出， 是一个特定的输出，对H取k个输入，至少有一个输入y使得时，k有多大？ y取k个随机值得到函数的k个输出中至少有一个等于H(x)的概率为：。若概率等于0.5，则。当输出长为m比特时， 第二类生日攻击： 寻找函数H的具有相同输出的两个任意输入 设哈希函数输出长度为m比特，H的k个随机输入中至少有两个产生相同输出的概率大于0.5，则： 11.3 迭代型哈希函数 输入消息M分为L个长度为b位的分组（），若不能正好分组，需要填充到b的整数倍位。f为压缩函数。 Hash函数的一般结构为： 迭代技术（压缩函数f）： 由于函数的输入包含了长度，所以攻击者必须：找出具有相同的Hash值且长度相等的两条消息，或者长度不等但是加入消息长度后Hash相同的消息，增加了攻击的难度。 Merkle和Damgard已经证明：如果压缩函数是无碰撞的，则上述方法得到的Hash函数也是无碰撞的。因此Hash函数的核心是设计无碰撞的压缩函数f，要求找出f的碰撞在计算上是不可行的。 11.3.1 MD5 迭代型散列函数 输入：任意长度。分组：512比特。输出：128比特。 算法步骤： 填充，填充的第一位是1，其余为0。填充到比512的倍数少64位（），原始已经满足长度要求的数据也需要填充。 附加消息长度：将原始消息长度用64位表示，先填充低32位，后填充高32位。长度超过64位的取低64位。经过1、2步预处理后，消息长度为512的倍数，分组： 初始化缓冲区：中间结果和最终结果都保存在128位的缓冲区里。缓冲区用4个32位寄存器表示。对4个缓冲区附初始值，并以小端格式存储。 以512位的分组为单位处理消息：迭代执行压缩函数： 由四轮组成，每轮中对四个缓冲区进行16次迭代，每步迭代中使用当前512比特分组中的一个字（32比特）。 每轮中使用当前分组的16个字的顺序不一样。每轮中使用的逻辑函数g执行的位运算也不同。常数表的作用是“随机化”32位的输入数据，消除输入数据的规律性。循环左移的值与迭代的部步数和轮数都有关。加法是模相加。 输出：第L个分组处理后就是x的散列值。 安全性：MD5杂凑码中输出的每一比特是所有输入比特的函数，因此获得了很好的扩散效果。 从穷举搜索的角度，第二类生日攻击需要进行次运算，因此认为MD5容易受到第二类生日攻击。 11.3.2 SHA-1 迭代型哈希函数 输入：长度小于的消息。分组大小：512比特。输出：160比特。 算法步骤： 填充，与MD5相同 附加消息长度：用64位表示长度，并以大端存储的方式附加在后面 初始化缓冲区：中间结果和最终结果都保存在160位的缓冲区里。缓冲区用5个32位寄存器表示。对5个缓冲区附初始值，并以大端格式存储。 以512位分组为单位处理消息：压缩函数： 共四轮处理，每轮20次迭代。 分组中前16个字直接使用，后面使用计算，输入分组的16个字扩展成80个字以供压缩函数使用。基本逻辑函数执行位运算，每轮不同。加法是模相加。 输出 安全性：SHA1抗穷举攻击的能力比MD5强，并且抗密码分析的能力不弱。 11.3.3 SM3 迭代型哈希结构 分组长度：512位。输出长度：256位。 算法步骤： 填充：填充到512位的倍数，方法与MD5一样。 消息扩展：对每个分组产生132个消息字，每个消息字长32位。 迭代压缩：使用消息字进行迭代。 输出 11.4 海绵结构：SHA-3 SHA-3 第三代安全散列算法，之前名为Keccak算法。 海绵函数允许输入长度和输出长度可变。 海绵结构包括两个阶段：吸水阶段和挤压阶段。 算法流程： 填充：消息n位，被分为k个长r的分组 对长度b=r+c的状态变量s进行操作，初值设为0，在迭代中更新。默认下c=1024, r=576 吸水阶段：填充c个零，将输入从r位扩展到b位；扩展后与s进行运算，作为迭代函数f的输入，输入结果为s的新值 挤压阶段：每轮迭代中s的前r位保留作为输出分组Zi 输出：输出数据块的个数j由需要输出的位数l决定： 11.5 基于分组密码的Hash函数 CBC和CFB工作模式的特点：一个明文块的改变，在加密时会引起相应的密文块及其后的所有密文块改变。因此可利用分组密码的CBC和CFB工作模式构造Hash函数。 构造方式： 链接变量作为密钥：上一轮的输出作为下一轮的密钥 基于密码分组链接(CBC)工作模式：上一轮的输出与下一组消息异或，然后被加密 基于密码反馈(CFB)工作模式：上一轮的输出在下一轮中被加密，解密结果与这组消息进行异或 基于分组密码CBC和CFB工作模式的Hash函数中的密钥k： 若k保密，则是带密钥的Hash函数，常用于产生MAC，用于保证消息的完整性。 若k公开，则此类Hash函数是不安全的，甚至不是弱无碰撞。 11.6 基于离散对数的Hash函数 Chaum-Heijst-Pfitzmann Hash函数： 基于离散对数问题，可以证明是安全的 p是一个大素数，是一个素数，a和b是Zp的两个本原元。定义hash函数为： Chaum-Heijst-Pfitzmann Hash函数是强抗碰撞的。 证明：用反证法。若Hash函数h有一对碰撞，则可证明离散对数能被有效计算。 12. 消息认证码 12.1 对消息认证的要求 验证的内容包括： 所收到的消息确实来自真正的发送方 消息没有被修改 也可以验证消息的顺序和及时性 12.2 消息认证函数 认证符的产生有三类： 哈希函数：消息映射为定长的哈希值，以哈希值作为认证符 消息加密：对整个消息加密后的密文作为认证符 消息认证码：是消息和密钥的函数，产生定长的值 利用对称密码进行认证： 发送端：将明文消息M作为某个函数F的输入，产生帧校验序列（FCS），M和FCS连接到一起后进行加密 接收端：解密，重新利用F函数计算M的FCS，如果计算得到和收到的FCS相等，则认为消息是真实的 公钥加密：先签名再加密：提供保密性和认证性 消息认证码MAC：是指消息被一密钥控制的公开函数作用后产生的、用作认证符的、固定长度的数值。。 MAC是实现有效、安全可靠数字签字和认证的重要工具，是安全认证协议中的重要模块。 MAC算法不要求可逆性，想是带有密钥的Hash函数。但是也不是加密函数，因为MAC算法不要求可逆性，与加密函数相比，MAC函数更不容易被攻破。 12.3 MAC的安全性 攻击目的： 伪造攻击：攻击者在没有密钥的情况下，伪造一个未经认证的对 密钥恢复攻击：攻击者通过分析一系列消息、认证码对，找到控制密钥 伪造攻击 密钥恢复攻击：考虑敌手使用穷搜索攻击获取密钥 密钥有种，MAC值有种，，敌手知道多对信息和其认证码 第一轮：对于，穷举所有的密钥，由于密钥数多于MAC值的取值，因此有个密钥可能是正确的 第二轮：对于，在上述密钥中遍历，找到也满足这个式子的密钥值，得到个密钥可能是正确的 不断重复：，则平均需要a轮 因此消息认证码的穷攻击比对使用相同长度密钥的加密算法的穷搜索攻击代价更大。 穷举攻击中，攻击者有通过攻击密钥空间和攻击MAC值两种办法 攻击密钥空间： 密钥长k位，对所有可能的密钥进行计算，至少有一个密钥会产生正确的MAC，代价为 根据上述的密钥恢复攻击，可知存在一些不是正确密钥的密钥也会导致某个信息的认证码是相同的，因此需要不断重复，使用别的消息进行检查。可以证明，检查这些消息总的代价是： 攻击MAC值： 目的是对给定的消息产生其有效的MAC或者对给定的MAC产生其给定的消息 与攻击具有单向性或抗弱碰撞能力的Hash码所需的代价相同，代价为 这种攻击方式不能离线进行 MAC的构造： 用hash函数构造MAC 用分组密码构造MAC 用伪随机算法构造MAC 12.4 基础Hash函数的MAC：HMAC HMAC能提供： 消息完整性认证 信源身份认证 HMAC的设计目标： 可以不经修改直接使用现有的哈希函数 镶嵌的哈希函数可以方便的被替换 保持镶嵌的散列函数的最初性能，不因用于HMAC而使其性能降低 以简单的方式处理密钥 在对镶嵌的散列函数合理假设的基础上，易于分析HMAC用于认证时的密码强度 -&gt; 是HMAC优于其他基于散列函数的MAC的一个主要方面，HMAC在其镶嵌的散列函数具有合理密码强度的假设下，可证明是安全的。 算法描述： L：M消息分组数，b：一个分组中的比特长度，n：哈希函数的输出长度 K密钥长度左边填充0到b比特长，如果K的长度大于b，则先对K进行哈希后填充。得到K' K'与ipad异或，后面链接M的分组 哈希上面的所有分组 K'与opad异或，将上一步的哈希链接在异或值后 进行hash，输出值为HMAC HMAC的有效实现： 12345678910111213141516def sha1_hmac(key, message: bytes): # 将密钥和消息分别进行填充 if len(key) &gt; 64: key = sha1(key) key += b'\\x00' * (64 - len(key)) ipad = b'\\x36' * 64 opad = b'\\x5c' * 64 # 图中虚线的部分可以提前计算，用于作为散列函数的初值IV k_ipad = bytes([x ^ y for x, y in zip(key, ipad)]) k_opad = bytes([x ^ y for x, y in zip(key, opad)]) # 计算HMAC-SHA1值 inner_hash = sha1(k_ipad + message) outer_hash = sha1(k_opad + inner_hash) return outer_hash HMAC的安全性 HMAC的安全取决于镶嵌的哈希函数的安全性。 证明了对HMAC的攻击等价于对内嵌散列函数的下述两种攻击之一： 攻击者能够计算压缩函数的一个输出，即使IV是随机的和秘密的。 攻击者能够找出散列函数的碰撞，即使IV是随机的和秘密的。 13. 数字签名 13.1 数字签名的基本概念 数字签名主要用于对数字消息进行签名，以防消息的冒名伪造或篡改，也可用于通信双方的身份鉴别。 需要具有可验证性和不可伪造性。 数字签名的主要功能： 确认信息是由签名者发送的 确认消息自签名后到收到为止，未被修改过 签名者无法否认签名是由自己发送的 一个数字签名包含三部分：密钥生成、签名算法、验证算法 数字签名的安全性： 四类攻击目标从前至后难度依次递减： 四类攻击能力从前至后逐渐增强： 数字签名的解决方案： 数字签名方案的分类： 13.2 常见的数字签名方案 13.2.1 ElGamal签名 随机数k不能泄露且不能被重复使用，否则x有可能被算出 存在ElGamal数字签名的变形在某些假设下，能被证明在选择消息攻击下是安全的 13.2.2 Schnorr数字签名 Schnorr数字签名方案的安全性分析 ElGamal系统中是中的本原元，而Schnorr中不是。因此ElGamal的安全性更高，因为 的阶更高。 Schnorr系统的签名文较短，e的长度由函数h决定 Schnorr首先提出可以提前计算，因此实现过程更快 13.2.3 ECDSA 13.2.4 SM2 椭圆曲线： 验证算法的特点：加入了较多的检错功能，防止信道干扰和对手的篡改。 目前尚没有发现求解椭圆曲线离散对数问题的亚指数算法。 软硬件实现规模小，容易实现160位的椭圆曲线密码的安全性，相当于1024位的 RSA密码。目前最大的应用是二代身份证。 14. 密钥的管理和分发 14.1 密钥管理 密钥管理原则： 攻击者无法窃取 即使窃取到了也无法使用（超过使用时间和范围限制） 密钥的分配和更新对用户透明 密钥产生： 长度应该足够 密钥生成算法要足够安全：密钥生成算法的安全性不应低于密码算法的安全性 避免使用弱密钥，应使用长密钥 密钥生成算法： 非线性密钥空间：算法生成的密钥的强度不一致 线性密钥空间：密钥强度一致 防止我方黑盒密码设备被敌方利用： 密钥分为前后两部分，前一部分是密钥，后一部分是一个固定值用该密钥加密后得到的字符串 设备执行时，先解密后面的字符串，若匹配则正常。若不匹配，使用另外一个弱算法。 密钥128位，识别串64位，随机选取一个得到好密钥的概率是 密钥验证：确认来自正确的人；确认密钥没有传输错误（附带一个用该密钥加密的密文） 密钥更新：用旧密钥计算、用旧密钥协商，新密钥的安全性不会超过旧密钥。重新认证身份并分发密钥。旧密钥必须销毁。 14.2 基于对称加密的对称密钥分发 两个用户使用单钥密钥体制的时候，必须先共享一个密钥，还需要经常更新密钥。 密码系统的强度也依赖于密钥分配技术。 密钥分配的基本方法： 密钥由A选取并通过物理手段发送给B 密钥由第三方选取并通过物理手段发送给AB 如果AB事先已有一密钥，则其中一方选取新密钥后，用已有的密钥加密新密钥并发送给另一方。 缺点：攻击者一旦获得一个密钥就可获取以后所有的密钥，同时用这种方法对所有用户分配初始密钥时，代价仍然很大。 如果AB与第三方分别有一保密信道，则第三方选取密钥后，分别在两个保密信道上发送给AB 第四种方法比较常用，其中的第三方通常是一个负责为用户分配密钥的密钥分配中心(KDC)。这时每一用户必须和密钥分配中心有一个共享密钥，称为主密钥。通过主密钥分配给一对用户的密钥称为会话密钥，通信完成后，会话密钥即被销毁。主密钥可通过物理手段发送。 密钥分配的具体过程： A向KDC发送 Request 和 N1（这次业务的唯一标识符，是一个随机数，防止重放和篡改） KDA返回一个由（A保存的主密钥的）加密的信息，保证只有A能解密。解密信息包括：，Request，N1， A得到会话密钥。并向B转发最后一项的内容。由于最后一项是由B的主密钥加密的，因此只有B可以解密。并且可以获取会话密钥和对话的另一方的ID B用会话密钥加密一个随机数N2发给A A以f(N2)返回作为对B的应答，f函数可以是对N2加一等操作。 45两步可以让B相信第三步收到的消息不是一个重放。第三步已经完成了密钥分配，45两步还完成了认证的功能。 密钥的分层控制： 网络中如果用户数目非常多且分布的地域非常广，则需要使用多个KDC的分层结构 分层结构可减少主密钥的分布，还可将虚假KDC的危害限制到一个局部区域，但会降低信任度 分布式密钥控制： 在上述方法中需要所有用户都信任KDC，还需要对KDC加以保护。如果密钥分配是无中心的，则可以不需要KDC，但是主密钥多达个。 在整个网络的局部范围却非常有用 密钥的控制使用： 主密钥：密钥加密密钥。会话密钥：数据加密密钥。 如果主密钥泄露了，则相应的会话密钥也将泄露，因此主密钥的安全性应高于会话密钥的安全性。 会话密钥的有效期： 会话密钥更换得越频繁，系统的安全性就越高。 会话密钥更换得太频繁，又将延迟用户之间的交换。 14.3 基于非对称加密的对称密钥分发 公钥用于分配单钥密码体制的密钥非常合适。 简单分配流程： 公钥缺少证书管理机构认证且非物理传输中容易受到中间人攻击。 具有保密性和认证性的密钥分配： 假定AB双方已完成公钥交换，则可按以下步骤建立共享会话密钥： 14.4 公钥分发 公开发布 公用目录表 公钥授权 优点：每次密钥的获得由公钥管理机构查询并认证发送，用户不需要查表，提高了安全性 缺点：公钥管理机构必须一直在线，由于每一用户要想和他人联系都需求助于管理机构，所以管理机构有可能成为系统的瓶颈。由管理机构维护的公钥目录表也易被敌手通过一定方式窜扰。 公钥证书 用户通过公钥证书来互相交换自己的公钥而无须与公钥管理机构联系 公钥证书由证书管理机构CA(certificate authority)为用户建立 A 将自己的公钥发给CA，CA返回A的公钥证书 X.509认证服务 证书机构Y颁发给用户A的证书表示为：。 V是版本号，SN是证书序号，AI是证书算法标识，CA是发行者，TA是有效期，A是用户信息，AP是A的公钥 任何可以访问CA的用户都可以得到一个证书，只有CA可以修改证书。由于证书用CA的私钥签名，不能伪造，因此可以放在一个公共目录中。 不共享同一CA的用户获得证书： 两个CA之间已经交换了公开密钥，含有对方的证书。则A可以先读取，获取X2的公钥后，在读取，读取B的公钥。 这个证书链可以表示为： 证书的撤销：每一个证书都具有一个有效期，有效期结束时证书将自动撤销。也可以在有效期结束前撤销。为了有效的管理证书的撤销，CA会维护一个证书撤销列表CRL。当用户获取一个证书时，应检查CA的CRL，判断该证书是否已被撤销。 14.5 公钥基础设施（PKI） 为管理公钥（生成、认证、存储、安装），须建立一套公钥基础设施（PKI，Public Key Infrastructure），PKI的基本组成元素是证书颁发机构（CA，Certificate Authority）。 PKI主要完成的工作为： 为用户生成一对密钥（公开密钥，私有密钥），并通过一定的途径分发给用户。 CA为用户签发数字证书，形成用户的公开密钥信息，并通过一定的途径分发给用户。 对用户证书的有效性进行验证。 对用户的数字证书进行管理。这些管理包括有效证书的公布、撤销证书的公布、证书归档等。 PKI包含五个关键元素： 端实体：一个在公钥数字证书作用范围中被认证的实体。 签证机构CA：证书和证书撤销列表的发行人。 注册机构RA：可选元素，承担一些签证机构（CA）的管理任务。 证书撤销列表发布点：可选元素，CA可通过它来发布证书撤销列表（CRL）。 证书存取库：必备元素，提供存取数字证书和证书撤销列表的方法。","link":"/2023/06/04/%E5%AF%86%E7%A0%81%E5%AD%A6%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/"},{"title":"秘密共享","text":"1. 基本概念 1.1 介绍 参考资料：《密码协议基础》第8章 1.2 符号介绍 ：全部参与的人数 ：门限值，即任意t个参与者可以恢复出秘密，但少于t个参与者无法计算出关于原秘密的任何信息 是q阶有限域，其中 为素数 ：要分享的秘密 ：秘密碎片 ：秘密分发者 ：参与者 2. Shamir 秘密分享体制 2.1 前置知识：Lagrange插值多项式 已知n+1个点x1，x2，…，xn的函数值，可以使用lagrange插值求出一个n次多项式插值函数f(x),f(x)是接近未知原函数p(x)的函数，根据插值函数f(x)求出p(x)的未知点. 拉格朗日插值基函数： 拉格朗日插值多项式公式： 2.2 秘密分发 D随机选择一个t阶多项式 ，并且 D将 发送给 2.3 秘密重构 利用Lagrange插值多项式可以计算出： 其中， 由于秘密是，则令，得到： 共t个参与者，参与者每个人计算： 恢复得到秘密： 3. 可验证秘密共享VSS 3.1 介绍 为解决不诚实分发中心的问题，秘密分发者不仅分发秘密的碎片，而且广播对秘密碎片的承诺，各成员收到碎片时，验证碎片是否正确，重构阶段也验证其他成员的秘密碎片的正确性。 因此相对于Shamir只能抵抗被动攻击，VSS可以抵抗分发者分发错误碎片和参与者提交错误碎片的主动攻击。 3.2 Feldman的VSS方案 为一个 大素数， 为 阶循环群， 为生成元 3.2.1 秘密分发 D选择一个随机多项式：，其中 ，各项系数均属于 。 D将 发送给 D广播承诺 参与者 收到自己的碎片 后，计算下式是否成立： 如果成立，表明该碎片有效；若不成立，则请求D重发正确的碎片 3.2.2 秘密重构 重构方法同Shamir 秘密分享体制，任意t个参与者向参与重构的其他合作者秘密广播自己的碎片，这样每个参与重构的成员都可以通过上述的方法验证所收到的碎片的有效性，并利用拉格朗日插值定理重构出秘密s。 3.3 Pedersen的VSS方案 Feldman的方案中将 也作为一个承诺发出，只是计算安全的。Petersen方案中进行了进一步的扩展，不会直接泄露秘密s，是无条件安全的。 为一个 大素数， 为 阶循环群， 为生成元， 是一个随机元素 3.3.1 秘密分发 D选取两个随机多项式$a(x) = {j = 0}{t-1}a_txt ,b(x) = {j = 0}{t-1}b_txt ，其中a_0 =s $ D将碎片 发给参与者Pi D广播承诺值 参与者 收到自己的碎片 后，计算下式是否成立： 如果成立，表明该碎片有效；若不成立，则请求D重发正确的碎片 3.3.2 秘密重构 重构方法同Feldman的方案。 4. 等长多项式承诺（Kate） 《Constant-Size Commitments to Polynomials and Their Applications》文献阅读笔记 4.1 介绍 对多项式承诺的两种方式： 对整个多项式进行承诺 即将这个多项式的系数通过某种方式链接在一起后进行承诺 但在揭示多项式中的某个点的值时会同时揭示整个多项式 对多项式的系数进行承诺 即对多项式的每个系数分别进行承诺 可以在不揭示整个多项式的情况下验证多项式在某个点的值是否与承诺一致，缺点是承诺的大小变大了。 4.2 定义 4.2.1 参数介绍 $$：安全参数 ：双线性配对群，其素数阶 4.2.2 多项式承诺 Setup()：生成公钥PK用于承诺，其中包含合适的代数结构e Commit()：使用PK对多项式进行承诺，输出承诺C和额外的信息d Open() VerifyPoly() CreateWitness()：是在i出的证据，输出 VerifyEval()：验证证据 4.3 PolyCommit_DL 计算安全 前提：在 上， 总能被 整除。 Setup() 和$_T 都是阶群，选择 的一个生成元g$ 为私钥，公钥为： Commit() 给定t次或小于t次的多项式，承诺。 由于，则承诺 Open() VerifyPoly()：验证 CreateWitness()：令，，输出 VerifyEval()：：验证 是C所承诺的多项式在下标i处的值。 4.4 PolyCommit_Ped 无条件安全 PolyCommit_DL是同态的，有多项式和，及其各自对应的承诺和。若，则，witness是 Setup() 和$_T 都是阶群，选择 的两个生成元g,h$ 为私钥，公钥为： Commit() 给定t次或小于t次的多项式，选择任意t次的随机多项式，承诺。 由于，则承诺 Open() VerifyPoly()：验证 CreateWitness()：令，，输出 VerifyEval()：：验证 是C所承诺的多项式在下标i处的值。 4.5 eVSS 在eVSS方案的Sh和Rec阶段，VSS方法与Feldman VSS方法完全相同，只是Feldman的t + 1个形如承诺被单个多项式承诺取代。 此外，除了共享si外，秘密发放者现在还向节点Pi发送一个见证wi。 总的来说，eVSS协议需要O(1)广播，而Feldman VSS需要O(n)广播。在多次指控的情况下，秘密发放者秘密发放者可以使用§3.4中描述的批量开启功能，为整个批次提供一个证人。此外，由于PolyCommit的同态性，eVSS方案可以很容易地转换为分布式密钥生成协议。 4.5.1 秘密分发 D选择一个次数为t的多项式，令，并广播承诺C=Commit() 对于，D计算秘密碎片 ，并CreateWitness()，输出通过安全可靠信道发送给Pi。 参与者Pi收到后，运行VerifyEval()。如果验证失败则向D发送验证失败的消息。 D如果收到收到大于t个VerifyEval验证失败的消息，则表明这个参与者中有过多的不诚实用户，是不合格的，无法继续进行。如果少于t个，则向报告失败的用户重发==（？）== 如果==revealed shares（？）== 在VerifyEval验证中失败，表明分发者D是不合格的。如果没有，表明各个参与者都接受了。 4.5.2 秘密重构 任何t + 1或更多的节点Pi发布他们接受的秘密碎片和见证。 所有t + 1(或更多)节点使用VerifyEval验证每个广播共享，然后插值对，确定秘密。 5. 高阈值异步可验证秘密共享（HAVSS） 《High-Threshold AVSS with Optimal Communication Complexity》文献阅读笔记 5.1 介绍 5.2 定义 5.2.1 算法定义 Setup Com：对要求次数，并输出一个承诺字符串 Eval：它输出一个包含i、计算值φ(i)和见证字符串wi的3元组。 Verify：输入y是Eval输出的一个三元组，进行验证 Hom：由于承诺的同态性质，输出的是 5.3 Haven的HAVSS协议，对一致采样的短秘密s 5.3.1 sharing phase 1）对秘密分发者D，收到 (ID.d, in, share, s) 随机生成多项式和承诺： 生成恢复多项式 R，次数为p，满足。 生成n个不同的随机的共享多项式S1, S2, ..., Sn，每个参与方都会收到其中要给，次数为t，满足。 计算R和Si多项式承诺： 。这些承诺用于检验多项式的一致性。 令。 构建验证所需数据：分发者生成检验需要的数据： 向量 ，其中包含第n个共享多项式 Si 在某个点的评估。这个向量中的值以转置的顺序排列，即第一个值是 S1(i)，第二个值是 S2(i)，以此类推。 n 个测试多项式 Ti，每个测试多项式是 R - Si，以证明份额多项式和恢复多项式之间的一致性，并计算 构建根承诺： 分发者构建了一个根承诺 ，这是一个向量承诺，包含了所有多项式承诺的信息。在后续的可靠广播协议中，服务器只会相信与根承诺 C 相关联的多项式承诺。并讲每个多项式的见证witness加入C中。 发送消息给参与方： 分发者向每个参与者发送：根承诺、恢复多项式的承诺、每个共享多项式的承诺$ ，每个人共享多项式见证_iS，以及所有测试多项式的评估T$。发送格式为：(ID.d, send, set_i) 2) 对参与者Pi，第一次从D收到消息 (ID.d, send, set_i) 时，进行回应 检验： 和 检验：恢复多项式的承诺和每个共享多项式的承诺都在跟承诺C中正确的位置上 检验： 向 发送：(ID.d, echo, info_ij) ，其中info中包含 重复上述操作， 3) 对参与者Pj从Pi第一次收到消息 (ID.d, echo, info_ij) 时 检验： 在C中的i位上，并且检验 如果自己还没有发送ready信息，并且已经收到了 2t+1 个有效的echo信息时：向所有人发送一个ready信息 (ID.d, ready, C) 4) 对参与者Pm第一次收到消息 (ID.d, ready, C) 如果自己还没有发送ready信息，等到收到 t+1 个他人发送来的ready信息后发送ready信息 如果已经收到了 2t+1 个ready信息了，则等到收到 t+1 个echo 信息时： 从 t+1 个有效的echo信息中对插值获得 计算 输出反馈信息：(ID.d, out, shared) 5.3.2 Reconstruction phase 1）receive (ID.d, in reconstruct) 向每个参与者Pj发送：(ID.d, reconstruct-share, , ) 2) receive (ID.d, reconstruct-share, , ) 如果 在C中并且验证： 如果接收到 p+1个有效的reconstruct-share 信息 则 利用 重构出R不等式，并输入 为秘密 6. 打包异步可验证秘密共享(PAVSS) 《Bingo: Adaptively Secure Packed Asynchronous Verifiable Secret Sharing and Asynchronous Distributed Key Generation》文献阅读 6.1 Introduction Bingo结合了所有这些增强功能：具体来说，它是一种自适应安全的打包异步可验证秘密共享(PAVSS)协议，该协议允许秘密发放者以个字的总通信复杂度共享f+1个秘密，其中n为各方的总数，f为恶意方的总数。此外，Bingo在假设n = 3f + 1时具有最佳弹性。","link":"/2023/08/24/%E7%A7%98%E5%AF%86%E5%85%B1%E4%BA%AB/"},{"title":"From Bingo to ADKG","text":"Bingo: ADKG_i () ：当前操作用户始终是用户 零、开始阶段： 初始化 prop dealer sigs 集合均为空 取任意的秘密值 作为dealer调用 BingoShare 分享 f+1 个秘密值 对于其他参与者 发来的 BingoShare 请求都进行参与 一、当用户 作为dealer完成 BingoShare 时： 将用户 添加到自己的集合 中 如果 集合中有f+1个用户，则 向所有人发送 proposal 消息： 二、如果从用户 处收到proposal 消息，则： 当集合 中每个用户k都作为dealer完成 BingoShare 时： 向参与者 发送signature消息： 三、如果从其他某个参与者 收到signature消息，则： 如果自己的 prop 集合不为空 并且使用参与者 验证这条 sig 消息正确，则： 将参与者 和他的签名信息添加到自己收集的签名集合 中 ==抗量子生成协议== 如果收集到了其他 f+1 个人的签名，则： 调用 ， 并且调用 checkValidity 校验有效性 四、当VABA终止并输出结果 时： 当集合 中每个用户k都作为dealer完成 BingoShare 时： 调用 五、当BingoSumExpAndRec终止并输出结果 时： 输出公钥 并终止 总结： 一旦某个参与方完成了至少 f+1 个消息发放者们的 BingoShare ，这个参与方就会要求另外 f+1 个参与方通过 proposal 消息验证这些 BingoShare 确实完成了。在完成所有这些消息发放者们的 BingoShare 后，各方在f + 1消息发放者们的集合上签字回复。 然后所有的参与者都会在一组有 f+1 个消息发放者们的集合上、和 f +1 个签字集合上达成一致，然后调用VABA。 上述过程正常情况下意味着至少有f+1方为消息发放者们提供了签名，那么至少有一个没有问题的参与方提供了签名。因此，这个没有错误的参与方完成了BingoShare，并且正常终止，每个没有错误的参与方最终也会这样做。各方然后等待 f+1 个消息发放者们的 BingoShare 完成。然后参与者 i 可以调用 BingoSumExpAndReci 输出pk和ski。 Bingo: VABA Leader Election() 注：当前操作用户始终是用户 零、开始阶段： 用户 初始化各集合为空集 用户 为每个其他参与方随机抽样一个秘密 作为 dealer 分三次调用 BingoShare 分享上述n个秘密 ==（为什么是3次，不懂？）== 对于其他参与者 发来的三次 BingoShare 请求都进行参与 一、如果完成了其他某个参与者 三次的BingoShare 请求，则： 将参与者 加入 集合中 如果 集合中有f+1个参与方，则 向所有人发送attach消息： 二、如果从其他某个参与者 收到attach消息，则： 如果收到的 集合是自己的 集合的子集，则 向参与者 发送signature消息： 三、如果从其他某个参与者 收到signature消息，则： 如果自己的 attached 集合不为空 并且使用参与者 验证这条sig 消息正确，则： 将参与者 和他的签名信息添加到自己收集的签名集合 中 如果收集到了其他 f+1 个人的签名，则： 调用 ， 并且调用 checkValidity 校验有效性 四、当Gather协议输出集合 时： 中每一项的下标，即 广播indices消息： 五、当第一次从其他某个参与者 收到indices消息 时： 调用 并得到结果 ，并且等待自己的 调用输出结果时， 对于 中的每个 ： ​ 对于 中的每个 ，以作为秘密发放者终止的所有BingoShare调用时：==== ​ 调用 六、如果BingoReconstructSum输出结果，则： 将 添加到集合 中 七、如果 不为空集，且 中每个参与方的评估结果都存在在 中时： 检查哪些参与方具有最大的关联值，并选择它作为领导者。==== Verifiable gather the goal of Gather is to have some common core gather-set such that all parties output a super-set of this core. For Verifiable Gather, the goal is to limit the power of the adversary to generate inconsistent outputs. Intuitively, for any gather-set produced by the adversary, if it passes some verification protocol, it must also be a super-set of the common core. 零、开始阶段： 初始化各集合 可靠广播消息 一、如果从用户 处收到消息 ，则： 将 添加到自己的 集合中，将用户 添加到自己的 集合中 如果 集合中已经有 个用户，则广播消息 二、如果从用户 处收到消息 ，并且 中元素个数大于 个，则： 当 收到的 是自己的 的子集的时候，即收到的集合中的用户的广播也已经被此用户收到时： 将用户 添加到自己的 集合中 如果 集合中已经有 个用户，则广播消息 三、如果从用户 处收到消息 ，并且 中元素个数大于 个，则： 当 收到的 是自己的 的子集的时候，即收到的集合中的用户的广播也已经被此用户收到时： 将 中每个用户 的 集合并起来，和用户 ，加入自己的集合 中 如果 集合中已经有 个用户，则输出集合 ，并且继续监听接受信息","link":"/2023/09/04/Bingo_VABA_Leader_Election/"},{"title":"数据库原理与安全：笔记","text":"第一章、绪论 1.1 数据库系统概述 （1）数据 描述事物的符号记录叫做数据（Data），是数据库中存储的基本对象。 数据的种类 结构化数据：可以使用关系型数据库进行表示和存储，可以表示为规范的二维表格形式。 半结构化数据：不符合关系数据模型结构，但包含相关标记用来分隔语义元素，并对记录和字段进行分层。例如XML、JSON格式数据。 非结构化数据：没有固定结构，如图片、视频等。 信息 = 数据 + 处理。 信息是具有时效性的，有一定含义的，有逻辑的、经过加工处理的、对决策有价值的数据流。 数据是符号化的信息；信息是语义化的数据。 举例： 学生档案中的学生记录（数据）：（李明，男，199505，江苏南京市，计算机系，2013） 语义：学生姓名、性别、出生年月、出生地、所在院系、入学时间 解释：李明是大学生，1995年5月出生，江苏南京市人，2013年考入计算机系 上述语义集成起来，就构成了“信息” 知识 数据挖掘 （2）数据库 数据库（Database，简称DB）是长期储存在计算机内、有组织的、可共享的大量数据的集合。 数据库的基本特征： 数据按一定的数据模型组织、描述和储存 可为各种用户共享 冗余度较小 数据独立性较高 易扩展 （3）数据库管理系统 数据库管理系统（DataBase Management System, DBMS）是位于用户与操作系统之间的一层数据管理软件，帮助用户定义、创建、维护和控制数据库访问的软件，是基础软件，是一个大型复杂的软件系统。 数据库管理系统DBMS在数据库建立、运用和维护时对数据库进行统一管理和控制，以保证数据的完整性、安全性，并具备多用户同时使用数据库的并发控制，在发生故障后对数据库进行恢复。 主要功能： 数据定义功能 提供数据定义语言（DDL）；定义数据库中的数据对象 数据组织、存储和管理 分类组织、存储和管理各种数据；确定组织数据的文件结构和存取方式；实现数据之间的联系；提供多种存取方法提高存取效率 数据操纵功能 提供数据操纵语言（DML）；实现对数据库的基本操作：增、删、改、查 数据库的事务管理盒运行管理 数据库在建立、运行和维护时，由DBMS统一管理和控制；保证数据的安全性、完整性；支持多用户对数据的并发使用；发生故障后的系统数据恢复 数据库的建立和维护功能 其他功能 （4）数据库系统 数据库系统（DataBase System, DBS），简称数据库 数据库系统的构成：数据库(Database)、数据库管理系统（DBMS及其应用开发工具）、应用程序(Application)、数据库管理员(DBA) （5）数据库系统的特点 数据结构化 这是数据库的主要特征之一，是数据库系统与文件系统的本质区别 数据内部和整体都是结构化的，数据之间是有联系的 数据的共享性高，冗余度低且易扩充 数据独立性高 物理独立性：应用程序和数据库中的数据物理存储是相互独立的。 逻辑独立性：应用程序和数据库中的数据的逻辑结构是相互独立的。 数据由DBMS统一管理和控制 安全性保护：防止数据被泄露和破坏 完整性保护：指数据的正确、有效、相容 并发控制 数据库恢复：有从错误状态恢复到某一正确、完整状态的功能 1.2 数据模型 数据","link":"/2023/09/16/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AC%94%E8%AE%B0/"},{"title":"网络内容安全笔记（1）","text":"第一章 网络信息内容获取技术 一、网络信息内容获取模型 1.1 网络信息内容获取模型 image-20231210155128073 1.2 网络媒体信息获取原理 image-20231210155651684 网络媒体信息获取的分类 全网信息获取 定点信息获取 基于主题的信息获取和元搜索 1.3 网络通信信息获取原理 image-20231210160648258 二、搜索引擎技术 中文搜索引擎的关键技术: 网页内容分析 网页索引 查询解析 相关性计算 2.1 网上采集算法（爬虫） image-20231210160930099 按照系统结构和实现技术，大致可以分为以下几种类型： 通用网络爬虫（General Purpose Web Crawler） 聚焦网络爬虫（Focused Web Crawler） 增量式网络爬虫（Incremental Web Crawler） 深层网络爬虫（Deep Web Crawler） 爬虫过程主要分为一下四步： 初始URL集合 信息获取：根据来自网络地址集合或URL队列中的每条网络地址信息，确定获取内容所采用的信息发布协议。基于特定协议的网络交互机制，向信息发布网站请求所需内容。 信息解析：从网络响应信息相应位置提取发布信息的主体内容（信息关键字段包括：信息来源、信息标题、信息失效时间、信息最近修改时间等等）。 信息判重：主要基于网络媒体信息URL与内容摘要两大元素，实现信息采集/存储的与否判断。 爬虫URL抓取策略： 深度优先遍历策略 宽度优先遍历策略 反向链接数策略：反向链接数表示的是一个网页的内容受到其他人的推荐的程度/引用的次数。因此，很多时候搜索引擎的抓取系统会使用这个指标来评价网页的重要程度，从而决定不同网页的抓取先后顺序。 Partial PageRank策略：对于于已经下载的网页，连同待抓取URL队列中的URL，形成网页集合，计算每个页面的PageRank值，计算完之后，将待抓取URL队列中的URL按照PageRank值的大小排列，并按照该顺序抓取页面 OPIC策略：在算法开始前，给所有页面一个相同的初始现金（cash）。当下载了某个页面P之后，将P的现金分摊给所有从P中分析出的链接，并且将P的现金清空。对于待抓取URL队列中的所有页面按照现金数进行排序。 大站优先策略：对于待抓取URL队列中的所有网页，根据所属的网站进行分类。对于待下载页面数多的网站，优先下载。 2.2 排级算法 网页排级是对搜索结果的分析，使那些更具“重要性”的网页在搜索结果中的排名获得提升，从而提高搜索结果的相关性和质量。 1. PageRank 原理：民主表决 核心思想： 在互联网上，如果一个网页被很多其它网页所链接，说明它受到普遍的承认和信赖，那么它的排名就高。. 假设互联网是一个有向图，在其基础上定义随机游走模型，即一阶马尔可夫链，表示网页浏览者在互联网上随机浏览网页的过程。假设浏览者在每个网页依照连接出去的超链接以等概率跳转到下一个网页，并在网上持续不断进行这样的随机跳转，这个过程形成一阶马尔可夫链。PageRank表示这个马尔可夫链的平稳分布。每个网页的PageRank值就是平稳概率。 对于网页关系： image-20231210162423298 有转移矩阵（随机游走模型，即一阶马尔可夫链）为： 随机游走在某个时刻 访问各个结点的概率分布就是马尔可夫链在时刻 的状态分布，可以用一个 维列向量 表示，那么在时刻 访问各个结点的概率分布 满足： PageRank 的基本定义： 给定一个包含 个结点的强连通且非周期性的有向图，在其基础上定义随机游走模型。假设转移矩阵为 ， 在时刻 访问各个结点的概率分布为： 则极限 存在，极限向量 表示马尔可夫链的平稳分布，满足 若随机游走的特点是从一个结点到有有向边连出的所有结点的转移概率相等，则该马尔科夫链有平稳分布 ，满足上式。 平稳分布 称为这个有向图的 PageRank；平稳分布 的各个分量称为各个结点的PageRank值。 其中满足： 这里 表示指向结点 的结点集合， 表示结点 连出的有向边的个数。 PageRank 的一般定义： 第二项称为平滑I页，由于采用平滑项，所有结点的 PageRank 值都不会为 0。并且所有结果和为1. 计算方法： 其中 是入度， 是出度， 是影响因子，取0.85。 为每个页面赋相同的初值，计算每个页面的PR值，应满足所有页面PR值和为1。若和不为1，则按比例对所有结果进行缩小，使得其和为1. 网页数量过大问题的解决方法： 稀疏矩阵 MapReduce 优点： 直接高效 主题集中 缺点： 完全忽略网页内容，干扰挖掘结果 结果范围窄 影响因子与网页获取数量缺乏科学性 2. HITS 在HITS算法中，每个页面被赋予两个属性：Hub属性和Authority属性。具有上述两种属性的网页分为两种：Hub页面和Authority页面。 Hub（枢纽）页面：类似于一个分类器，其为包含了很多指向高质量Authority页面链接的网页。例如，hao123首页汇集了全网优质网址，故可以认为其是一个典型的高质量Hub网页； Authority（权威）页面：类似于一个聚类器，其为与某个领域或者某个话题相关的高质量网页。例如，京东首页、淘宝首页等，都是与网络购物领域相关的高质量网页。 枢纽值（Hub Scores）页面上所有导出链接指向页面的权威值之和。 权威值（Authority Scores）所有导入链接所在的页面的枢纽值之和。 HITS算法的目的即是通过一定的技术手段，在海量网页中找到与用户查询主题相关的高质量Authority页面和Hub页面，尤其是Authority页面，因为这些页面代表了能够满足用户查询的高质量内容，搜索引擎以此作为搜索结果返回给用户。 HITS算法基于两个重要的假设： 一个高质量的Authority页面会被很多高质量的Hub页面所指向； 一个高质量的Hub页面会指向很多高质量的Authority页面。 算法步骤： 接受一个用户查询的请求。 将查询提交给某个现有的搜索引擎，并在返回的搜索结果中，提取排名在前n（n一般取200左右）的网页，得到一组与用户查询高度相关的初始网页集合，这个集合被称作为 root set（即根集） 在根集的基础上，将与根集内网页有直接链接指向关系的网页（每个网页取d个，d一般取50左右）扩充进来形成base set（即扩展集）。 计算最终集中所有页面的Hub值和Authority值。 依据Authority值对页面进行排序，取值较高的前若干位返回作为用户查询结果的响应。 缺点： 计算效率低，实时性差 “主题漂移” 易被作弊者操纵结果：作弊者可以建立一个很好的Hub页面，再将这个网页链接指向作弊网页，可以提升作弊网页的Authority得分 结构不稳定：在原有的“扩充网页集合”内，如果添加删除个别网页或者改变少数链接关系，则HITS算法的排名结果就会有非常大的改变。 优点： 知识范围扩大。 搜索时部分地考虑了页面内容，挖掘结果科学性大大增强。 3. 比较 HITS算法是与用户输入的查询请求密切相关的，而PageRank与查询请求无关。所以，HITS算法可以单独作为相似性计算评价标准，而PageRank必须结合内容相似性计算才可以用来对网页相关性进行评价。 HITS算法因为与用户查询密切相关，所以必须在接收到用户查询后实时进行计算，计算效率较低；而PageRank则可以在爬虫抓取完成后离线计算，在线直接使用计算结果，计算效率较高。 HITS算法的计算对象数量较少，只需计算扩展集合内网页之间的链接关系；而PageRank是全局性算法，对所有互联网页面节点进行处理。 从两者的计算效率和处理对象集合大小来比较，PageRank更适合部署在服务器端，而HITS算法更适合部署在客户端。 HITS算法存在主题泛化问题，所以更适合处理具体化的用户查询；而PageRank在处理宽泛的用户查询时更有优势。 2.3 搜索引擎与垃圾信息关系 搜索引擎优化（Search Engine Optimization）：利用工具或其他手段，使目标网站符合搜索引擎的搜索规则，从而获得较好的排名。 SEO分为两类： 优化网站内容，提升网页质量 构造垃圾信息 垃圾信息制造手段包括： 提高排名（Boosting）技术 关键字垃圾（term spamming） 链接垃圾（link spamming） 隐藏（Hiding）技术 对所使用的Boosting技术进行隐藏，尽量不让用户和网络采集器发现 主要技术包括内容隐藏（content hiding）、伪装（cloaking）和重定向（redirection） 如何提高PR： 提升网页质量 登录搜索引擎和分类目录；以及友情链接，如果能获得来自PR值不低于4并与你的主题相关或互补的网站的友情链接，且很少导出链接，那样效果更好 写一些高质量的软文，发布到大型网站，如果得到大家的认可，你的网址会被无数的网站转载 搜索引擎收录一个网站的页面数量，如果收录的比例越高，对提高PR值越有利 把所有页面都提交给Google 制作一个网站地图/导航，包含所有要添加的网站，然后提交网站地图 三、数据挖掘技术 从大量非结构化、异构的Web信息资源中发现兴趣性（interestingness）的知识，包括概念、模式、规则、规律、约束及可视化等形式的非平凡过程. image-20231210170657144 image-20231210170707640 image-20231210170720314 四、信息推荐技术 信息推荐算法： 基于内容推荐：根据用户已选择的对象，推荐其他类似属性的对象作为推荐。 协同过滤推荐：推荐相似用户所选择的对象 启发式方法：使用与新用户c相似的用户c’对一个对象的评价来预测s对新用户c的效用，进而判断是否推荐s给c。 基于模型的方法：利用用户c对众多对象的评分来学习一个c的模型，然后使用概率方法对新的对象s的推荐效用进行预测。 组合推荐 后融合组合推荐：融合两种或两种以上的推荐方法各自产生的推荐结果，判断使用其中的哪个推荐结果更好。属于结果层次上的融合 中融合组合推荐：以一种推荐方法为框架，融合另一种推荐方法。 前融合组合推荐：直接融合各种推荐方法。 五、信息还原技术 电脑还原技术 软件还原 硬件还原 网页还原技术 数据包捕获技术：网络数据包的捕获技术采用的网卡接收方式为混杂方式 协议还原技术 网页内容还原技术 多媒体信息还原技术. 六、课后题： 1. 各技术详细介绍 交互（Interaction）: 是指人与计算机或其他设备之间的通信和数据交换。在软件设计中，良好的交互设计可以提高用户体验和效率。 信息浏览（Information Browsing）: 指用户在互联网或内部系统上查找和浏览信息的过程。有效的信息浏览依赖于用户界面设计和搜索算法的优化。 GYM: 通常指代互联网领域的三大巨头：谷歌（Google）、雅虎（Yahoo）、微软（Microsoft）。这些公司在搜索引擎、在线服务和软件开发等多个方面具有显著影响。 URL（Uniform Resource Locator）: 是一种网络资源的地址格式，用于在互联网上定位和访问网页、图片、视频等资源。 Crawler（网络爬虫）: 是一种自动访问网页并从中提取信息的程序，常用于搜索引擎索引、数据分析和网络内容监控。 Sniffer（网络嗅探器）: 是一种监测和分析网络流量的工具，用于网络安全、故障诊断和网络性能分析。 Promiscuous（混杂模式）: 是指网络设备（如网卡）接收并处理经过其网络接口的所有数据包，而不仅是发往该设备的数据包。这在网络监控和分析中很有用。 Web挖掘（Web Mining）: 指从网络数据（如网页内容、用户行为数据）中提取有用信息和模式的技术，可用于市场分析、搜索引擎优化等领域。 SEO（Search Engine Optimization，搜索引擎优化）: 指通过优化网站结构和内容，提高网站在搜索引擎中的排名和可见性的技术。 数据挖掘（Data Mining）: 是从大量数据集中通过算法分析提取有价值信息和隐藏模式的过程，广泛应用于商业、科研等领域。 2. 信息获取技术分类及详细说明 信息获取技术可以分为以下几个主要类别： 数据采集技术：包括网络爬虫、API调用等，用于从互联网或其他数据源收集信息。 数据处理技术：涉及数据清洗、数据转换等，用于提高数据质量和便于进一步分析。 信息检索技术：包括搜索引擎技术、数据库查询等，用于从大量数据中查找特定信息。 数据分析技术：如数据挖掘、统计分析等，用于从数据中提取洞见和知识。 用户界面技术：涉及交互设计、信息可视化等，用于改善用户获取和处理信息的体验。 3. 信息内容获取模型详细内容 信息内容获取模型通常包含以下步骤： 需求分析：确定用户或系统需要什么样的信息，包括信息的类型、深度和用途。 信息源定位：识别和选择合适的信息源，如网络、数据库或其他数据存储。 信息获取：实际从信息源中获取信息，可能通过搜索、订阅、数据采集等方式进行。 信息处理：对获取的信息进行加工和整理，包括过滤无关信息、数据格式转换等。 信息存储：将处理后的信息储存于数据库或其他存储媒介，以便后续使用。 信息展示和传递：将信息以合适的方式呈现给用户，可能包括图表、报告或交互界面等。 4. PageRank核心原理及排名过程 PageRank是基于网页之间链接关系的一种算法，用于衡量网页的重要性。其核心原理和排名过程包括： 链接投票：一个网页链接到另一个网页，被视为对后者的“投票”或推荐。 重要性传递：重要的网页链接到的网页也被认为更重要。 排名计算：通过迭代计算所有网页的PageRank值。每个网页的PageRank值是所有链接到它的网页的PageRank值的总和，按链接数量和质量加权。 迭代更新：这个过程反复进行多次，直到所有网页的PageRank值达到稳定。 5. 信息还原技术的各方面 信息还原技术主要包括： 数据恢复：从损坏、格式化或者删除的存储设备中恢复数据。 错误检测与纠正：利用算法检测数据传输或存储过程中的错误，并进行修正。 信息重构：在部分数据缺失或损坏的情况下，尝试基于现有数据和模式重建原始信息。 备份和恢复策略：定期备份重要数据，并在数据损坏时进行恢复。 文件系统修复：修复损坏的文件系统，恢复文件系统的结构和存储的数据。 第二章 文本挖掘基础：文本预处理 一、文本挖掘的背景 image-20231210211057717 二、分词 2.1 数字的识别 使用正则表达式识别（有限状态自动机） 2.2 词语标记（Tokenization）算法 算法过程： 指针从左到右扫描字符char，存储在W[i]中，i++ 如果当前指针位置char是分隔符 并且W不是空格，则将分隔符之前的内容输出为一个单词，并从句子中删除整个单词，并清空W 如果W是空格，则清空W，并从句子中删除这个空白字符 2.3 词性还原（Lemmatization）算法： 屈折型语言的词的变化形式： 屈折变化：词性不变，但形式发生变化，例如动词的过去式、单三等 派生变化：词性变化，一个单词从另外一个不同类单词或词干衍生过来。 符合变化：两个或更多个单词以一定的方式组合成一个新的单词。 需要： 词典（Dict） 前缀表（PrefixList） 后缀表（SuffixList） 有关屈折词尾变形的规则（Rules） 算法过程： 待分析的词语W，单词字符数为d，i=1 在词典（Dict）中查找单词W，如果找到，直接输出 如果i&lt;=(d/2) 从W中取出i个尾字符串，W成为两部分W1+W2 到后缀表（SuffixList）中查找W2，如果查到，调用规则，对W1进行处理，得到W1' 到Dict中查找W1'，如果找到，输出W1' 和 W2 i=i+1 2.4 汉语分词 汉语分词面临的问题： 重叠词、离合词（担心：担什么心）、词缀 汉语的切分歧义 交集型歧义（交叉型歧义）：如果字串abc既可切分为ab/c，又可切分为a/bc。 有意见： 我 对 他 /有/意见。 总统/有意/见他。 组合型歧义（覆盖型歧义）：若ab为词，而a和b在句子中又可分别单独成词。 – 马上：我/马上/就 来。 他/从/马/上/下来。 混合型歧义 汉语的未登录词（词库中没收录的词语） 1. 正向最大匹配法（MM） 正向即从左往右取词，取词最大长度为词典中长词的长度，每次右边减一个字，直到词典中存在或剩下1个单字。 举例： sentence = '我们在野生动物园玩' user_dict = ['我们', '生动', '野生', '动物园', '野生动物园', '物','玩'] 词典最大长度 max_len = 5 匹配过程： 最大长度为5，从头取出5个字“我们在野生”，然后进行扫描 第1次：“我们在野生”，扫描词典，无 第2次：“我们在野”，扫描词典，无 第3次：“我们在”，扫描词典，无 第4次：“我们”，扫描词典，有 扫描中止，输出第1个词为“我们”，去除第1个词后，开始第2轮扫描 取出“在野生动物”进行扫描 第1次：“在野生动物”，扫描词典，无 第2次：“在野生动”，扫描词典，无 第3次：“在野生”，扫描词典，无 第4次：“在野”，扫描词典，无 第5次：“在”，只剩下一个字，输出 扫描中止，输出一个字为“在”，去除第2个词后，开始第3轮扫描 取出”野生动物园“，在字典中，输出 取出”玩“，只有一个字，输出 结果为：“我们/在/野生动物园/玩” 如果词典中有词语”在野“，则分词结果为：“我们/在野/生动/物/园/玩” 2. 逆向最大匹配法（RMM） 逆向即从后往前取词，其他逻辑和正向相同。 例如上个例子中，先取出”生动物园玩“，从前往后删除字，“动物园玩”、“物园玩”、“园玩”、“玩”，取出“玩” 3. 双向最大匹配 依次采用正向最大匹配和反向最大匹配 如果结果一致则输出 如果结果不一致，则采用其他方法排歧 正向最大匹配法，最终切分结果为：“我们/在野/生动/物/园/玩”，其中单字字典词为2，非词典词为1。 逆向最大匹配法，最终切分结果为：“我们/在/野生动物园/玩”，其中，单字字典词为2，非词典词为0。 非字典词：正向(1)&gt;逆向(0)（越少越好） 单字字典词：正向(2)=逆向(2)（越少越好） 总词数：正向(6)&gt;逆向(4)（越少越好） 因此最终输出为逆向结果。 最大匹配法分词的问题： 最大词长的确定 词长过短，长词会被切错（“中华人民共和国”） 词长过长，效率就比较低 掩盖了分词歧义问题，没有处理 能发现部分交集型歧义，无法发现组合型歧义 对于某些交集型歧义，可以增加回溯机制来改进最大匹配法，对不在字典的单字进行进一步处理 4. 最大概率法分词 一个待切分的汉字串可能包含多种分词结果，将其中概率最大的作为该字串的分词结果。 例句：“有意见分歧” 对候选词wi的累计概率： 左邻词：“意见”和“见”都是“分歧”的左邻词 最佳左邻词：某个候选词有若干个左邻词，其中累计概率最大的为最佳左邻词。 算法过程： 对一个待分词句子，从左到右取出所有候选词 查出每个候选词的概率，并记录每个候选词的全部左邻词 计算每个候选词的累计概率，得到最佳左邻词 到达尾词w_n，并且w_n的累计概率最大，则w_n是终点词 从终点词开始向前，输出每个词的最佳左邻词，则为分词结果 缺点： 并不能解决所有的交集型歧义问题 无法解决组合型歧义问题 三、文档模型 1. 布尔模型 建立在经典的集合论和布尔代数的基础上，每个词在一篇文档中是否出现，对应权值为0或1 文档检索→布尔逻辑运算 优点：简单、易理解、简洁的形式化。 缺点：准确匹配，信息需求的能力表达不足。 2. 词袋模型 在信息检索中，BOW模型假定对于一个文档，忽略它的单词顺序和语法、句法等要素，将其仅仅看作是若干个词汇的集合，文档中每个单词的出现都是独立的，不依赖于其它单词是否出现。 Wikipedia上给出了如下例子: John likes to watch movies. Mary likes too. John also likes to watch football games. 根据上述两句话中出现的单词, 我们能构建出一个字典 (dictionary): {\"John\": 1, \"likes\": 2, \"to\": 3, \"watch\": 4, \"movies\": 5, \"also\": 6, \"football\": 7, \"games\": 8, \"Mary\": 9, \"too\": 10} 该字典中包含10个单词, 每个单词有唯一索引, 注意它们的顺序和出现在句子中的顺序没有关联. 根据这个字典, 我们能将上述两句话重新表达为下述两个向量: [1, 2, 1, 1, 1, 0, 0, 0, 1, 1] [1, 1, 1, 1, 0, 1, 1, 1, 0, 0] 这两个向量共包含10个元素, 其中第i个元素表示字典中第i个单词在句子中出现的次数. 因此BoW模型可认为是一种统计直方图 (histogram). 在文本检索和处理应用中, 可以通过该模型很方便的计算词频. 词袋模型中，向量中可以使用1/0值表示是否出现，可以使用词频，也可以填入TF-IDF值。 3. n-gram模型 ？（ngram不能向量化的表示一个文章） 4. TF-IDF算法 词频（TF）= 某词在文章中出现的总次数 / 文章的总词数 逆文档频率（IDF）= log（语料库的文档总数 / （包含该词的文档数 + 1）） TF-IDF = TF × IDF 四、文档相似度计算 4.1 基于概率模型的相关度 BM25算法 4.2 基于向量空间模型的相关度 SVM 4.3 基于向量内积的相关度 1. 欧式距离 2. 向量内积相似度 3. 余弦相似度 4. Jaccard相似度 4.4 基于文本序列的相关度 序列比较 海明距离 编辑距离 第三章 文本分类 文本分类的基本步骤： 一、评价指标 准确率：预测正确的样本总数 ÷ 总样本数 精确率（Precision）：预测正确的样本数 ÷ 预测出来的样本数，即 召回率（Recall）：预测正确的样本数 ÷ 标志的样本数（即事实上正确的样本数），即 F1： 宏平均：把每个类别都当作正类计算一下precision，recall，f1然后求和求平均 微平均： 二、特征选择 2.1 文档频率（DF: Document Frequency） TF-IDF算法 TF-IDF算法的主要思想是：如果某个词或短语在某一篇文章中的出现频率TF越高，而且在其它文章中很少出现，那么认为此词或者短语具有很好的类别区分能力，适合用来分类。 在统计之前必须要过滤掉文档中的停用词。当然TF-IDF的精确度有时候可能不太高，它仍有不足之处，单纯地认为文本频率越小的单词就越重要，而文本频率越大的单词就越无用，显然这并不完全正确。 计算出文档中每个词的TF-IDF的值，然后按照降序排列，取前面的几个词作为特征属性。这里由于只取前K大的，有比较优秀的O(n)算法。 在文本分类中单纯地用TF-IDF来判断一个特征属性是否具有区分度是不够的，原因主要有如下两个： 没有考虑特征词在类间的分布 如果一个特征词在各个类之间分布都比较均匀，那么这样的词对分类没有任何贡献；而如果一个特征词集中分布在某个类中，在其它类中都出现但是出现的频率很小很小，那么这个词能很好地代表这个类的特征属性，但是TF-IDF不能很好地区别这两种情况。 没有考虑特征词在类内部文档中的分布 在类内部文档中，如果特征词均匀分布在其中，那么这个特征词能够很好地代表这个类的特征，如果只在几篇文档中出现，那么不能够代表这个类的特征。 2.2 信息增益（IG: Information Gain）? 条件熵：在某一条件下，随机变量的不确定性。 信息增益：在某一条件下，随机变量不确定性减少的程度。 在信息增益中，重要的衡量标准就是看这个特征能够为分类系统带来多少信息，带来的信息越多，那么该特征就越重要。通过信息增益选择的特征属性只能考察一个特征对整个系统的贡献，而不能具体到某个类别上，这就使得它只能做全局特征选择，即所有的类使用相同的特征集合。 2.3 互信息（MI: Mutual Information） 互信息是事件A和事件B发生相关联而提供的信息量，在处理分类问题提取特征的时候就可以用互信息来衡量某个特征和特定类别的相关性，如果信息量越大，那么特征和这个类别的相关性越大。反之也是成立的。 其中X，Y分别对应词项t和类别出现的情况c（有若干种可能，下表中是二元模型，即只有两种可能） 特点： MI(t,c) 的值越大，t对C的区分能力越强 同一个类中，相对稀有的词t会得到较大的值 一个词如果频次不够多，但是又主要出现在某个类别里，那么就会出现较高的互信息，从而给筛选带来噪音。 所以为了避免出现这种情况可以采用先对词按照词频排序，然后按照互信息大小进行排序，然后再选择自己想要的词，这样就能比较好的解决这个问题。 2.4 卡方（CHI） 卡方检验是数理统计中一种常用的检验两个变量是否独立的方法。在卡方检验中使用特征与类别间的关联性来进行量化，关联性越强，特征属性得分就越高，该特征越应该被保留。 卡方检验最基本的思想是观察实际值和理论值的偏差来确定理论的正确性。通常先假设两个变量确实是独立的，然后观察实际值与理论值的偏差程度，如果偏差足够小，那么就认为这两个变量确实是独立的，否则偏差很大，那么就认为这两个变量是相关的。 三、分类算法 3.1 KNN分类 输入没有标签的新数据后，将新数据与样本集中数据进行比较，然后算法提取样本集中最相似数据（最近邻）的分类标签。 一般来说，只选择样本数据集中前N个最相似的数据。K一般不大于20，最后，选择k个中出现次数最多的分类，作为新数据的分类。 距离计算：方式有多种，比如常见的曼哈顿距离计算、欧式距离计算等等。 欧式距离计算公式： KNN 算法最简单粗暴的就是将预测点与所有点距离进行计算，然后保存并排序，选出前面 K 个值看看哪些类别比较多。 K值的选择：通过交叉验证（将样本数据按照一定比例，拆分出训练用的数据和验证用的数据，比如6：4拆分出部分训练数据和验证数据），从选取一个较小的 K 值开始，不断增加 K 的值，然后计算验证集合的方差，最终找到一个比较合适的 K 值。 优点： 简单、有效 重新训练方便 计算时间和空间的规模与训练集规模成线性关系 KNN是一种非参的（不会对数据做出任何的假设）、惰性的（没有明确的训练数据的过程）算法模型。 缺点： 对内存要求较高，因为该算法存储了所有训练数据。 预测阶段可能很慢。 对不相关的功能和数据规模敏感。 3.2 贝叶斯分类 贝叶斯公式： 类别特征特征类别类别特征 假设每个样本数据由n维特征向量 表示，且有m个类别，由 表示。朴素贝叶斯分类器的目标是对于一个给定的未知类别的样本 ，为其分配具有最高后验概率的类 在朴素贝叶斯分类中，“朴素”的条件独立假设为：假设每个特征对于其他特征是独立的，即特征之间相互独立。则 在文本分类中，对一个文档X可能的类别均计算贝叶斯概率，其中分母都是带分类的文章的特征向量，因此只需比较分子即可。 3.3 SVM分类 1. SVM介绍 支持向量机（support vector machines, SVM）是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；SVM还包括核技巧，这使它成为实质上的非线性分类器。SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。 分类： 线性可分支持向量机：硬间隔最大化 线性支持向量机：训练数据近似线性可分时，通过软间隔最大化 非线性支持向量机：当训练数据线性不可分时，通过使用核技巧(kernel trick)及软间隔最大化 2. SVM算法原理 SVM学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。 参考：【机器学习】支持向量机 SVM - 知乎 (zhihu.com) （1）间隔最大化 超平面可表示为： 点 到超平面的距离是： 支持向量到超平面的距离为 ，其他点到超平面的距离大于 对于每个数据：，有： 将移动到分母后，是一个正数，对优化无影响，所以可以省略，两个式子合并后得到： 支持向量到超平面的距离可以写为： 为了最大化这个距离，则需要最小化，等价的，需要最小化 因此得到最优化问题： （2）对偶问题 得到带有约束条件的问题： 引入松弛变量得到，则得到 Lagrange 函数： 其中引入了n个Lagrange 乘子，和n个松弛变量。 则原来的 转化为 问题。 对求导，得到极值点出的必要条件： 其中第二个式子 有两种情况： $ _i = 0, a_i ，约束条件g_i(w)不起作用，且g_i(w)&lt;0$ ，约束条件起作用，且 则综合可得：，即支持向量的，其他向量的 由此方程组转换为，得到不等式约束优化优化问题的 KKT条件： 的最后一项中 ，则进一步转化为 ： 由于 ，则 ，为了找到最优的参数 ，使得 接近目标，故问题转换为 那么最优化问题转换为： 拉格朗日函数对偶性，将最小和最大的位置交换一下，得到： 3. SVM求解过程 主要问题是： （） 求解线性可分的 SVM 的步骤为： 构造拉格朗日函数： 利用强对偶性转化： 对参数 w 和 b 求偏导数： 得到： 带回函数中，得到： 即： 得到问题： 这是一个二次规划问题，问题规模正比于训练样本数，我们常用 SMO(Sequential Minimal Optimization) 算法求解。通过 SMO 求得最优解 。 求偏导数时得到： 由上式可求得 w。 并且所有 对应的点都是支持向量，我们可以随便找个支持向量，然后带入：，求出 b 即可： w 和 b 都求出来了，我们就能构造出最大分割超平面： 分类决策函数： 其中sign为阶跃函数： 4. SVM算法过程（PPT） 特征空间上的训练集： 线性可分向量机的目标：找到分离的超平面并获得决策函数 函数间隔： 给定一个训练样本 x是特征，y是结果标签，i表示第i个样本。定义函数间隔为：。 由于是一个训练样本，那么为了使函数间隔最远（更大的信息确定该样本是正例还是反例），所以，当yi = 1时，我们希望w ∗ xi + b能够非常大，反之是非常小。因此函数间隔代表了我们认为特征是正例还是反例的确信度。 函数间隔如果同时扩大w和b的话，点之间的距离会扩大，但是超平面没有变化。 几何间隔：，表示了一个训练样本到超平面的距离。所以SVM实际的目的时找到函数间隔最小的那对样本，并且要让它的几何间隔最大。 两者的关系： 最大间隔分类的超平面，就是要找到： 由函数间隔和几何间隔的关系，可以得到上式等价于： 其中取，最大化 和最小化 等价，因此线性可分支持向量机学习的最优化问题为： 在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量(support vector)；支持向量是使约束条件式等号成立的点。 构造拉格朗日函数： 由于是对偶问题，从极小极大问题变为极大极小问题： 先求对w,b的极小值，对w,b分别求偏导，令等于0，得到： 带入原式中，得到： 对${w,b}L(w,b,) 对求极大，则是对 -{w,b}L(w,b,) $求极小，得到表达式： 定理：若 是上述式子的解，则存在，可以求得原始问题的解（偏导第一个式子，和最优化问题的取等条件）： 因此，分离超平面可以写成： 5. 软间隔最大化 6. 非线性支持向量机与核函数 非线性问题往往不好求解，所以希望能用解线性分类间题的方法解决这个问题。 采取的方法是进行一个非线性变换，将非线性问题变换为线性问题，通过解变换后的线性问题的方法求解原来的非线性问题。 原空间： 新空间： 分隔平面： 核技巧：通过一个非线性变换将输入空间(欧氏空间或离散集合)对应于一个特征空间(希尔伯特空间)，使得在输入空间中的超曲面模型对应于特征空间中的超平面模型(支持向量机)。分类问题的学习任务通过在特征空间中求解线性支持向量机就可以完成 核函数：设X是输入空间(欧氏空间R^n的子集或离散集合)，又设H为特征空间(希尔伯特空间)，如果存在一个从X到H的映射：，对所有，函数满足：，则称函数为核函数，该映射为映射函数。 使用代替目标函数和决策函数中的，即得到核函数的目标函数和决策函数。 常用核函数： 多项式核函数（Polynomial kernel function）： 对应的支持向量机为P次多项式分类器 高斯核函数 （Gaussian Kernel Function）/ 径向核函数： 字符串核函数 7. 序列最小最优化算法 (SMO) 解如下凸二次规划的对偶问题：（变量是拉格朗日乘子αi，一个对应一个样本） 启发式算法，基本思路: 如果所有变量的解都满足此最优化问题的KKT条件，那么得到解 否则，选择两个变量，固定其它变量，针对这两个变量构建一个二次规划问题，称为子问题，可通过解析方法求解 子问题的两个变量：一个是违反KKT条件最严重的那个，另一个由约束条件自动确定：","link":"/2023/12/10/%E7%BD%91%E7%BB%9C%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8/"},{"title":"网络内容安全笔记（2）","text":"第四章 文本挖掘：聚类 一、聚类方法概述 1.1 概述 聚类：聚类(clustering)也称为聚类分析,指将样本分到不同的组中使得同一组中的样本差异尽可能的小，而不同组中的样本差异尽可能的大。 聚类得到的不同的组称为簇(cluster)。 一个好的聚类方法将产生以下的聚类： 最大化类中的相似性 最小化类间的相似性 聚类和分类的区别： 聚类的样本不具有类别标号，是无监督学习，可以先聚类，再对有限的簇指定类别 分类的样本有类别标号，是有监督学习，需要高昂的代价标记训练样本 1.2 聚类分析中的数据类型 文本向量模型 / 数据矩阵（Data matrix，或称对象-属性结构） 用 p 个变量（也称为属性）来表现 n 个对象，这种数据结构是关系表的形式，或者看为 n×p 维（ n 个对象*p 个属性）的矩阵。 相异度矩阵（dissimilarity matrix 或称对象-对象结构） 存储 n 个对象两两之间的近似性，表现形式是一个 n×n 维的矩阵。在这里 d(i,j)是对象 i 和对象 j 之间相异性的量化表示，通常它是一个非负的数值，当对象 i 和j 越相似，其值越接近 0；两个对象越不同，其值越大。既然 d(i,j) = d(j,i)，而且 d(i,i)=0，我们可以得到如下的矩阵。 1.3 相异度计算 1. 区间标度变量 两个点：与 欧几里得距离 两个元素在欧氏空间中的集合距离 曼哈顿距离 在曼哈顿街区要从一个十字路口开车到另一个十字路口，驾驶距离显然不是两点间的直线距离。这个实际驾驶距离就是“曼哈顿距离”。曼哈顿距离也称为“城市街区距离”(City Block distance)。 民科夫斯基距离 明氏距离是一组距离的定义，公式中其中p为一个变参数，当p=1时，就是曼哈顿距离；当p=2时，就是欧式距离；当p→∞时，就是切比雪夫距离。 余弦相似度 余弦度量的是两者的相似度，cosθ余弦值的取值范围[-1,1]。夹角余弦越大，表示两个向量的夹角越小，越相似；夹角余弦越小，表示两个向量之间的夹角越大，越不相似。 2. 二元变量 只有两个状态（0或1）的变量称为二元变量。二元变量分为对称的和非对称的两种。如果假设所有的二元变量有相同的权重，则可以得到一个两行两列（2*2）的条件表。 对于对称的二元变量，两个状态是等价值的，有相同的权重，例如性别中男性和女性，则相异度计算为： 对于不对称的二元变量，两个状态的权重不同，例如疾病的阳性和阴性，我们把重要的状态编码为1，次要的为0。此时两个都取1的匹配比两个都取0的匹配更有意义。 此时相异度计算为： 相似度计算为：（Jaccard系数） 3. 分类变量 / 标称变量 对二元变量进行拓展，属性的取值为多种，而不是只有1/0。相异度计算为： 其中p为全部变量的数量，m为i和j匹配的数目（取相同状态的数目） 4. 序数变量 / 顺序变量 对分类变量进行拓展，属性取值有若干种，并且这些状态可以依据某标准进行排序。 则计算相异度时，先将状态f替换为它的秩（它排序的序号1,2,3,...,M），然后将秩映射到[0,1]区间上，可以通过下面变换实现： 其中， 是第i个对象的第f个变量的秩， 是 变量f的取值状态的数目。 接下来就可以用区间标度变量中所描述的任意一组距离度量方法进行计算相异度。 5. 比例标度变量 属性的取值随时间呈指数增长趋势，例如： 计算相异度时可以： 直接当作区间标度变量处理，但是不好，比例尺度是非线性的。 把比例标度度量当做序数变量处理 对比例标度度量做对数变换： 6. 混合变量模型 相异度计算： 其中， 是单个类型变量定义的距离；p是变量的个数。 二、划分聚类方法 给定一个有n个对象的数据集，划分聚类技术将构造数据k个划分，每一个划分就代表一个簇。也就是说，它将数据划分为k个簇，而且这k个划分满足下列条件： 每一个簇至少包含一个对象。 每一个对象属于且仅属于一个簇。 2.1 聚类的差异 类内差异 类内差异度量了一个簇内部对象之间的相似性或紧密度。通常，我们希望簇内的对象越相似越好，这意味着在同一个簇中的对象应该彼此接近。 类内差异可以通过计算簇内对象之间的平均距离或方差来衡量。较小的类内差异值表示簇内的对象更相似，簇内更紧密。例如： 类间差异 类间差异度量了不同簇之间的分离性或差异性。它表示不同簇之间的对象应该尽可能地不相似，这有助于区分不同的簇。 类间差异可以通过计算不同簇之间的平均距离或方差来衡量。较大的类间差异值表示不同簇之间的对象更不相似，簇间更分离。例如： 聚类的总体质量可以定义为 和 的单调组合，例如 2.2 k-means 算法 K均值聚类（K-Means Clustering）是一种常用的划分聚类算法，它被用来将数据集分成不同的簇。这个算法的主要思想是将数据点划分为k个簇，每个簇的中心是该簇内所有数据点的均值。 算法基本步骤： 初始化：首先，选择要分成的簇数k。然后从数据集中随机选择k个点作为初始簇的中心点，这些点可以是数据集中的实际数据点或者随机生成的点。 分配数据点：对于每个数据点，计算它与每个簇中心的距离，通常使用欧氏距离或其他距离度量。将数据点分配给距离最近的簇中心，使得它属于该簇。 更新簇中心：对于每个簇，计算簇内所有数据点的均值，并将该均值作为新的簇中心。 重复步骤2和步骤3：重复执行分配数据点和更新簇中心的步骤，直到满足停止条件。常见的停止条件包括簇中心不再发生明显变化，或者指定的迭代次数已达到。 输出结果：算法结束后，每个数据点都被分配到一个簇中。这就是K均值聚类的最终结果。 优点： 简单且易于理解：K均值聚类是一个直观和易于理解的算法。它的基本原理是直观的，容易实现和解释，因此适用于初学者和非专业人士。 计算效率高：K均值聚类在大规模数据集上运行效率很高，因为它的时间复杂度通常是线性的，而且算法迭代次数通常较少。 适用于均匀分布的簇：当数据的簇是凸形的、球形的或近似球形的时候，K均值聚类的效果通常很好。 缺点： 需要定义平均值：在簇的平均值被定义的情况下才能使用，可能不适用于某些应用。 需要预先指定簇数k：在使用K均值聚类之前，需要预先知道要分成多少个簇。选择不合适的k值可能导致聚类结果不准确。有一些方法可以帮助估计合适的k值，但仍然需要主观决策。 假设簇是凸形的：K均值聚类假设簇是凸形的，对于非凸形的簇效果较差。 不适用于不均匀大小和形状的簇：如果簇的大小和形状差异很大，K均值聚类可能会产生不理想的结果。 对异常值敏感：K均值聚类对异常值（离群点）敏感，异常值可能会影响簇的中心位置，从而导致不正确的聚类结果。 k-中心点算法： k-means算法对于孤立点是敏感的。为了解决这个问题，引入了k-中心点算法，该算法不采用簇中的平均值作为参照点，可以选用簇中位置最中心的对象，即中心点作为参照点。 2.3 PAM算法 PAM（Partitioning Around Medoids）算法是一种聚类算法，它是K均值聚类的一种改进版本，旨在解决K均值聚类对离群点和异常值敏感的问题。与K均值聚类不同，PAM算法使用数据点作为中心点，而不是数据的均值，这使得它对离群点更具鲁棒性。 k-medoids修正聚类中心的时候，是计算类簇中除开聚类中心的每点到其他所有点的聚类的最小值来优化新的聚类中心。正是这一差别使得k-mediod弥补了k-means算法的缺点：k-mediod对噪声和孤立点不敏感。 算法流程为：z 在总体n个样本点中任意选取k个点作为medoids 按照与medoids最近的原则，将剩余的n-k个点分配到当前最佳的medoids代表的类中（实现了初始的聚类） 对于第 i 个类中除对应medoids点外的所有其他点，按顺序计算当其为新的medoids时，准则函数的值，遍历所有可能，选取准则函数最小时对应的点作为新的medoids 重复2-3的过程，直到所有的medoids点不再发生变化或已达到设定的最大迭代次数 产出最终确定的k个类 准则函数： 最小化绝对误差： 另一种表达： k‐均值迭代计算簇的中心的过程，在PAM算法中对应计算是否替代对象o′比原来的代表对象o能够具有更好的聚类结果，替换后对所有样本点进行重新计算各自代表样本的绝对误差标准。 若替换后，替换总代价小于0，即绝对误差标准减小，则说明替换后能够得到更好的聚类结果，若替换总代价大于0，则不能得到更好的聚类结果，原有代表对象不进行替换。 在替换过程中，尝试所有可能的替换情况，用其他对象迭代替换代表对象，直到聚类的质量不能再被提高为止。(贪心算法) 2.4 谱聚类算法 谱聚类算法 - 知乎 (zhihu.com) 算法流程： 优点： 谱聚类只需要数据之间的相似度矩阵，因此对于处理稀疏数据的聚类很有效。这点传统聚类算法比如K-Means很难做到。 由于使用了降维，因此在处理高维数据聚类时的复杂度比传统聚类算法好。 缺点： 如果最终聚类的维度非常高，则由于降维的幅度不够，谱聚类的运行速度和最后的聚类效果均不好。 聚类效果依赖于相似矩阵，不同的相似矩阵得到的最终聚类效果可能很不同。 三、层次聚类方法 基于层次的聚类方法是指对给定的数据进行层次分解，直到满足某种条件为止。该算法根据层次分解的顺序分为自底向上法和自顶向下法，即凝聚式层次聚类算法和分裂式层次聚类算法。 层次凝聚的代表是AGNES算法。层次分裂的代表是DIANA算法。 3.1 AGENS算法 AGNES 算法最初将每个对象作为一个簇，然后这些簇根据某些准则被一步步地合并。两个簇间的相似度有多种不同的计算方法。聚类的合并过程反复进行直到所有的对象最终满足簇数目。 算法步骤： 输入：包含n个对象的数据库。 输出：满足终止条件的若干个簇。 过程： 初始化：将每个对象当成一个初始簇。 REPEAT 计算任意两个簇的距离，并找到最近的两个簇。 合并两个簇，生成新的簇的集合。 UNTIL 终止条件得到满足。 两个簇之间的距离计算方式： 最小距离：两个簇中任意两个元素之间的距离的最小值 若采用最小距离的定义，簇与簇的合并方式称为单链接方法. 最大距离，距离的最大值 均值距离：两个簇中分别求的平均元素，然后做差 平均距离：两个簇中任意两个元素距离的平均值 终止条件： 设定一个最小距离阈值D，如果最相近的两个簇的距离已经超过D，则它们不需再合并，聚类终止。 限定簇的个数k，当得到的簇的个数已经达到k，则聚类终止。 缺点： 不可逆性：AGNES算法的一大缺点是一旦完成了两个簇的合并，就无法撤销这个操作。因此，如果在早期阶段错误地合并了两个不相似的簇，将会对最终的聚类结果产生较大的影响。这种不可逆性使得AGNES算法对于选择合适的距离度量和簇合并策略变得非常关键。 计算复杂性：AGNES算法在每次迭代中都需要计算所有对象两两之间的距离。这导致了算法的时间复杂度为O(n^2)，其中n是数据点的数量。因此，当数据集很大时，计算距离矩阵和查找最近簇的成本会很高，因此该算法对于大规模数据集不太适用。 对初始簇数的敏感性：AGNES算法的性能也受到初始簇数的影响。不同的初始簇数和初始簇合并顺序可能会导致不同的最终聚类结果。因此，选择合适的初始条件对于获得理想的聚类结果非常关键。 不适用于非凸簇：与K均值聚类不同，AGNES算法假定簇可以是任意形状，这在某些情况下可以是一个优点。然而，对于非凸形状的簇，AGNES算法也可能产生不太理想的结果。 3.2 DIANA算法 DIANA（Divisive Analysis）算法是一种层次聚类算法，与AGNES（Agglomerative Nesting）算法相反，它是自顶向下的方法。DIANA算法的主要目标是将数据集分割成多个子集，每个子集表示一个簇，然后继续将这些子集进一步划分，直到每个簇包含一个数据点。 算法步骤： 输入：包含n个对象的数据库。 输出：满足终止条件的若干个簇。 过程： 初始化：将所有对象整个当成一个初始簇。 REPEAT 在所有簇中挑出具有最大直径的簇C。 找出簇C中与其它点平均相异度最大的一个点p，并把p放入splinter group，剩余的放在old party中。 REPEAT 在old party中选择一个点q。 计算点q到splinter group中的点的平均距离D1，计算点q到old party中的点的平均距离D2，保存D2-D1的值。 选择D1-D2取值最大的点q'，如果D1-D2为正，把q'分配到splinter group中。 UNTIL 没有新的old party的点被分配给splinter group。 splinter group和old party为被选中的簇分裂成的两个簇，与其它簇一起组成新的簇集合。 END. 简单的来说： 初始化，所有样本集中归为一个簇 在同一个簇中，计算任意两个样本之间的距离，找到 距离最远 的两个样本点a,b，将 a,b 作为两个簇的中心; 计算原来簇中剩余样本点距离 a，b 的距离，距离哪个中心近，分配到哪个簇中 重复步骤2、3 …… 直到，最远两簇距离不足阈值，或者簇的个数达到指定值，终止算法 3.3 Birch算法 BIRCH聚类算法原理 - 刘建平Pinard - 博客园 (cnblogs.com) BIRCH（Balanced Iterative Reducing and Clustering using Hierarchies）算法是一种层次聚类算法，旨在处理大规模数据集。BIRCH的主要优势在于它能够高效地处理大量数据并生成紧凑的数据摘要，同时保持聚类质量。 Birch算法是层次聚类算法之一，该算法引入了聚类特征和聚类特征树(Clustering Feature Tree)。 聚类特征树的每个节点都可以用它的聚类特征(CF)表示,形式为，N为簇中样本的个数，LS为n个点的线性和，SS为样本的平方和。 CF有一个很好的性质，就是满足线性关系，即： CF树的参数： B：每个非叶子节点最多有B个分支 L：每个叶子节点最多有L个CF 分支 T：每个CF分支的直径不超过阈值T 算法可以分为两个步骤： BIRCH扫描数据库，建立一颗存放于内存的CF-树，它可以看作数据的多层压缩，试图保留数据内在的聚类结构。 BIRCH采用某个(选定的)聚类算法对CF-树的叶子节点进行聚类，把稀疏的簇当做离群点删除，而把稠密的簇合并为更大的簇。 CF-树的构建流程： 从根节点开始：插入新样本时，从CF树的根节点开始搜索。 寻找最近的叶子节点和最近的CF节点：递归向下搜索，找到距离新样本最近的叶子节点以及叶子节点中最近的CF节点。这一步是为了确定新样本的插入位置。 检查CF节点的超球体半径是否满足阈值T：如果新样本被插入后，该CF节点对应的超球体半径（表示簇的范围）仍然满足小于阈值T，则说明可以将新样本加入到这个CF节点中，而无需进行分裂。在这种情况下，需要更新路径上所有的CF三元组以反映新样本的插入。 如果超球体半径不满足阈值T：如果插入新样本后，CF节点的超球体半径超过了阈值T，则需要进行分裂操作。 检查叶子节点的CF节点数量是否小于阈值L：如果当前叶子节点的CF节点数量小于阈值L，说明可以将新样本插入到当前叶子节点中。创建一个新的CF节点，将新样本放入该CF节点，然后将新的CF节点放入当前叶子节点。最后，需要更新路径上所有的CF三元组以反映新样本的插入。 如果CF节点数量已达到阈值L：如果当前叶子节点的CF节点数量已经达到阈值L，说明叶子节点已满，需要将叶子节点分裂为两个新的叶子节点。分裂过程如下： 选择旧叶子节点中超球体距离最远的两个CF元组，这两个CF元组将成为两个新叶子节点的第一个CF节点。 将其他CF元组和新样本元组按照距离远近的原则放入对应的新叶子节点。 然后，依次向上检查父节点是否也需要分裂，如果需要分裂，则按照与叶子节点相同的方式进行分裂。 结束插入操作：插入完成后，CF树的结构和数据摘要已更新，插入操作结束。 BIRCH算法主要流程 将所有的样本依次读入，在内存中建立一颗CF Tree，它可以看作数据的多层压缩，试图保留数据内在的聚类结构。 （可选）将第一步建立的CF Tree进行筛选，去除一些异常CF节点，这些节点一般里面的样本点很少。对于一些超球体距离非常近的元组进行合并。 （可选）利用其它的一些聚类算法比如K-Means对所有的CF元组进行聚类，得到一颗比较好的CF Tree.这一步的主要目的是消除由于样本读入顺序导致的不合理的树结构，以及一些由于节点CF个数限制导致的树结构分裂。 （可选）利用第三步生成的CF Tree的所有CF节点的质心，作为初始质心点，对所有的样本点按距离远近进行聚类。这样进一步减少了由于CF Tree的一些限制导致的聚类不合理的情况。 从上面可以看出，BIRCH算法的关键就是步骤1，也就是CF Tree的生成，其他步骤都是为了优化最后的聚类结果。 在构造完成CF-树以后，BIRCH采用某个(选定的)聚类算法对CF-树的叶子节点进行聚类，把稀疏的簇当做离群点删除，而把稠密的簇合并为更大的簇。这些聚类算法可以是k-means，或者分层凝聚法等。 BIRCH算法试图利用可用的资源来生成最好的聚类结果。通过一次扫描就可以进行较好的聚类，故该算法的计算复杂度是O(n)，n是对象的数目。 优点： 节约内存，所有的样本都在磁盘上，CF Tree仅仅存了CF节点和对应的指针。 聚类速度快，只需要一遍扫描训练集就可以建立CF Tree，CF Tree的增删改都很快。 可以识别噪音点，还可以对数据集进行初步分类的预处理。 缺点： 由于CF Tree对每个节点的CF个数有限制，导致聚类的结果可能和真实的类别分布不同. 对高维特征的数据聚类效果不好。此时可以选择Mini Batch K-Means。 如果数据集的分布簇不是类似于超球体，或者说不是凸的，则聚类效果不好。 四、密度聚类方法 密度聚类方法的关键思想是通过检测局部密度变化来识别簇的边界，而不仅仅依赖于全局距离度量。密度聚类方法的指导思想是，只要一个区域中的点的密度大于某个域值，就把它加到与之相近的聚类中去。这使得密度聚类方法对不同形状、大小和密度的簇具有良好的适应性，并且对于噪声数据具有一定的鲁棒性。 4.1 DBSCAN算法 Visualizing DBSCAN Clustering (具象化演示网站) DBSCAN（Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法）是一种基于密度的空间聚类算法。 该算法将具有足够密度的区域划分为簇，并在具有噪声的空间数据库中发现任意形状的簇，它将簇定义为密度相连的点的最大集合。 一些重要概念： ε-邻域（Epsilon Neighborhood）：ε-邻域是指以某个数据点为中心，半径为 ε 的圆形区域，包括了距离该点在 ε 范围内的所有数据点。ε-邻域是用来衡量数据点之间距离的局部范围。 核心对象（Core Object）：在DBSCAN中，核心对象是指在 ε-邻域内至少包含 MinPts 个数据点的数据点。核心对象是密度高的数据点，是簇的中心。 直接密度可达（Directly Density-Reachable）：如果数据点 A 在数据点 B 的 ε-邻域内，且数据点 B 是核心对象，那么数据点 A 被称为直接密度可达于数据点 B。这意味着数据点 A 可以通过 ε-邻域内的密度较高的核心对象 B 直接到达。但是不能说B可以通过A直接到达，除非A也是核心对象。即密度直达是不对称的。 密度可达（Density-Reachable）：如果存在一条数据点序列 P1, P2, ..., Pn，其中 P1 = A，Pn = B，且对于任意相邻的数据点 Pi 和 Pi+1，Pi+1 是从 Pi 直接密度可达的，那么数据点 A 被称为密度可达于数据点 B。这表示通过一系列直接密度可达的数据点可以连接起点 A 和终点 B。密度可达也是不对称的。 密度相连（Density-Connected）：如果存在一个数据点 C，使得数据点 A 和数据点 B 都是密度可达于数据点 C，那么数据点 A 和数据点 B 被称为密度相连。密度相连是一种传递性的关系。密度相连时对称的。 简要步骤： DBSCAN算法根据密度可达关系求出所有密度相连样本的最大集合，将这些样本点作为同一个簇。DBSCAN算法任意选取一个核心对象作为“种子”，然后从“种子”出发寻找所有密度可达的其他核心对象，并且包含每个核心对象的ε-邻域的非核心对象，将这些核心对象和非核心对象作为一个簇。当寻找完成一个簇之后，选择还没有簇标记的其他核心对象，得到一个新的簇，反复执行这个过程，直到所有的核心对象都属于某一个簇为止。 具体步骤： 初始化：选择两个参数，Epsilon（ε）和MinPts。 ε（Epsilon）：表示半径，它用于定义一个数据点的邻域范围。 ε 决定了数据点在 ε-邻域内有多少个邻居。 MinPts：表示在 ε-邻域内至少应包含的数据点数量。MinPts 用于确定核心点（密度足够高的点）。 选择起始点：从数据集中随机选择一个数据点作为起始点。 找到 ε-邻域：计算起始点到数据集中所有其他数据点的距离，并将距离小于 ε 的数据点视为起始点的 ε-邻域内的邻居。 核心点检测：如果 ε-邻域内的邻居数量不少于 MinPts，则将起始点标记为核心点。 扩展簇：如果起始点是核心点，那么开始从它的 ε-邻域内扩展簇。具体步骤如下： 将起始点添加到当前簇中。 对于 ε-邻域内的每个点，如果它是核心点，则将它的 ε-邻域内的邻居点也添加到当前簇中。 递归地继续扩展簇，直到没有更多的核心点可以添加。 找到下一个起始点：如果当前簇已扩展完毕，则从数据集中选择下一个未访问的核心点作为新的起始点，然后重复步骤 3 到步骤 5。 标记噪声点：所有未被分配到任何簇的数据点被视为噪声点。 结束：当所有数据点都被分配到簇或标记为噪声点时，DBSCAN算法结束。 DBSCAN的输出是一组簇，每个簇包含若干核心点和它们的密度可达的数据点，以及一些噪声点。DBSCAN的主要优点包括对不同形状和大小的簇具有良好的适应性，能够自动确定簇的数量，以及对噪声数据点有一定的鲁棒性。这使得它在实际应用中广泛使用，尤其适用于发现具有不规则形状的聚类结构。 优点： 能够自动识别不同形状和大小的簇：DBSCAN不依赖于簇的形状和大小，能够识别具有不规则形状的聚类结构。这使得它在实际应用中非常灵活。 对噪声数据有较好的鲁棒性：DBSCAN能够自动将噪声数据点识别为孤立点或离群点，而不将其分配给任何簇。 不需要提前指定簇的数量：与许多其他聚类算法不同，DBSCAN无需事先指定要找到的簇的数量，它能够根据数据的本地密度自动确定簇的数量。 缺点： 对参数敏感：DBSCAN需要用户事先指定两个关键参数：ε（邻域半径）和 MinPts（核心点最小邻居数）。不同的参数选择可能导致不同的聚类结果，因此参数的选择需要谨慎，可能需要进行试验和调整。 可能产生边界点和孤立点：DBSCAN将数据点划分为核心点、边界点和噪声点，但边界点有时可能与多个簇关联，导致歧义。此外，DBSCAN可能将簇分成多个部分，这取决于参数的设置和数据的分布。 不适用于数据密度差异很大的情况：如果数据集中存在密度差异非常大的区域，DBSCAN可能不适用，因为 ε 和 MinPts 参数需要在整个数据集上保持一定的一致性，这可能会导致无法识别低密度区域的簇。 4.2 OPTICS算法 （4）聚类算法之OPTICS算法 - 知乎 (zhihu.com) 机器学习笔记（十一）聚类算法OPTICS原理和实践_optics聚类-CSDN博客 在 DBSCAN 算法中，邻域参数(E，MimPts)是全局唯一的，当样本点的密度不均匀或聚类间相差很人时，聚类质量较差。此外，DBSCAN 算法对邻域参数(E，MinPts)非常敏感，需要由用户指定参数，参数设置的不同可能导致聚类结果差别很大，当用户不了解数据集特征时，很难得到良好的聚类结果。为了克服这些缺点，安克斯特(Ankerst) 等人提出了 OPTICS 算法。 OPTICS 也是基于密度的聚类算法，但这一算法生成一个增广的簇排序，即所有分析对象的线性表，代表各样本点基于密度的聚类结构。从线性表的排序中可以得到基于任何邻域参数(E，MinPts)的 DBSCAN 算法的聚类结果。 重要概念： 核心距离：对于一个给定的核心点X，使得X成为核心点的最小邻域距离 r 就是X的核心距离。 可达距离：假设 q 是核心点。那么点 p 和点 q 的可达距离定义为： 即，如果X是核心对象，则对象 Y 到对象 X 的可达距离就是Y到X的欧氏距离和X的核心距离的最大值，如果X不是核心对象，则Y和X之间的可达距离就没有意义（不存在可达距离）。 算法流程： 输入： 数据集 D。 输出： 有序的输出结果队列 R，以及相应的可达性距离。 初始化：创建两个队列，有序队列 O 和结果队列 R。 如果数据集 D 中还有未处理的点或者存在核心点，则执行以下步骤： 选择一个未处理且为核心对象的样本点 P。 将 P 放入结果队列 R 中，并从数据集 D 中删除 P。 找到样本点 P 在数据集 D 中的所有密度直达样本点 X，并计算 X 到 P 的可达距离。 如果 X 不在有序队列 O 中，则将 X 及其可达距离放入 O 中。 如果 X 已经在 O 中，检查如果 X 的新可达距离更小，则更新 X 的可达距离。 对有序队列 O 中的样本点按可达性距离从小到大进行排序。 如果有序队列 O 为空，则跳至步骤 1 如果有序队列 O 不为空，则执行以下步骤： 取出 O 中的第一个样本点 Y（即可达性距离最小的样本点）。 将 Y 放入结果队列 R 中，并从数据集 D 和有序队列 O 中删除 Y。 如果 Y 不是核心对象，则执行以下步骤： 重复步骤 5.1（即找 O 中剩余数据可达性距离最小的样本点）。 如果 Y 是核心对象，则执行以下步骤： 如果 Y 已经在结果序列中，则不处理 找到 Y 在数据集 D 中的所有密度直达样本点（如果某些点已经在结果序列中则跳过），并计算到 Y 的可达距离。 按照步骤 2 将所有 Y 的密度直达样本点更新到 O 中。 重复以上步骤，直到算法结束 通过分析输出结果队列 R 中的数据点顺序以及它们的可达性距离，可以确定数据点之间的聚类结构。不同密度级别的簇会以不同的形式出现在结果队列中。通常，连续的数据点（可达性距离较小的数据点）表示同一簇内的点，而可达性距离较大的数据点表示不同簇之间的分隔点。 4.3 DENCLUE算法 DENCLUE（Density-Based Clustering of Applications with Noise）是一种基于密度的聚类算法，用于在数据集中发现聚类簇和噪声点。该算法通过建模数据点的局部密度分布来识别聚类簇，并使用核密度估计来确定数据点的聚类结构。以下是DENCLUE算法的关键特点和步骤： 其核心思想可以概括为以下几点： 影响函数（Influence Function）： DENCLUE将每个数据点的影响（影响其邻域的程度）形式化建模为一个数学函数，称为影响函数。这个函数描述了一个数据点对其周围邻域的影响程度，通常是一个随距离递减的函数。影响函数的选择可以根据问题的特性和数据的性质进行调整。 整体密度建模： DENCLUE通过将所有数据点的影响函数叠加在一起来建模数据空间的整体密度。这表示整体密度是由每个数据点的局部影响共同构成的，而不是简单地假定所有数据点具有相同的密度。这使得DENCLUE能够捕获数据集中不同区域的密度差异。 密度吸引点（Density Attractor）： 簇可以通过寻找全局密度函数的局部最大值来确定。在DENCLUE中，这些局部最大值被称为密度吸引点。密度吸引点表示数据集中的密度高点或局部聚类中心，因为它们是整体密度函数的局部极大值。DENCLUE的目标之一是识别这些密度吸引点，因为它们对应于聚类簇的核心。 优点： 对不规则形状和不均匀密度的聚类适应性好： DENCLUE能够发现各种形状的聚类簇，包括不规则形状和密度不均匀的簇，因为它基于数据点的局部密度分布进行建模。 对噪声点具有鲁棒性： DENCLUE可以有效地将噪声点识别为未连接到任何密度吸引点的数据点，这使得它对数据中的噪声具有较好的鲁棒性。 对密度变化敏感： 由于DENCLUE使用核密度估计来描述数据点的局部密度，因此它对密度的变化和梯度变化敏感，能够在数据集中捕获密度的细微差异。 缺点： 计算复杂度高： DENCLUE的计算复杂度较高，特别是在大规模数据集上运行时，需要处理大量的局部密度估计和梯度计算，因此在处理大数据时可能效率较低。 对参数敏感： DENCLUE需要设置一些参数，如影响函数和吸引子生成的停止条件。选择适当的参数值可能需要一些经验或试验，不合适的参数值可能会影响聚类结果。 五、其它聚类方法 5.1 WaveCluster：小波变换聚类 WaveCluster 是一种基于小波变换的聚类算法，旨在处理具有噪声和异常值的数据集。该算法利用小波变换技术来提取数据的特征，从而能够识别出不同尺度和密度的聚类簇。以下是 WaveCluster 算法的主要特点和步骤： 主要特点： 小波变换： WaveCluster 使用小波变换来分析数据的频率和尺度特征。小波变换是一种信号处理技术，可将数据转换为不同尺度上的频域信息，从而更好地捕获数据中的结构。 自适应阈值： WaveCluster 使用自适应阈值来检测噪声和异常值。这有助于排除不符合聚类结构的数据点，提高聚类的鲁棒性。 多尺度聚类： WaveCluster 能够识别不同尺度上的聚类结构，因此对于具有多个尺度的数据集特别有用。它可以处理具有不同密度和尺度的簇。 算法步骤： 小波变换： 将数据应用小波变换，将数据转换为小波域，得到小波系数。这些小波系数包含了不同尺度和频率上的数据特征。 聚类分析： 对小波系数进行聚类分析，以识别具有相似特征的数据点。通常，使用一些聚类方法来对小波系数进行聚类，例如K均值聚类。 噪声过滤： 使用自适应阈值来过滤掉被认为是噪声的数据点，以提高聚类的准确性。 簇合并： 将具有相似特征的小波系数聚合到一起，以形成最终的聚类簇。 可视化和分析： 最后，对聚类结果进行可视化和分析，以了解数据的聚类结构和特征。 小波变换在聚类中是有用的： 提供无指导聚类： 小波变换可以提供无指导聚类，因为它通过帽形过滤（wavelet thresholding）可以强调点密集的区域，并忽视在密集区域以外的较弱信息。这有助于识别数据集中的聚类结构，而无需事先指定簇的数量或形状。 有效去除孤立点： 小波变换可以有效地去除孤立的离群点或噪声，因为这些点通常在小波变换后的系数中表现为低幅度的信号，可以通过阈值处理来剔除。 多分辨率： 小波变换是一种多分辨率分析方法，可以捕获数据在不同尺度上的特征。这允许小波变换识别具有不同尺度的聚类结构，从细粒度到粗粒度。 有效花销： 小波变换的计算复杂性通常是线性的，因此在一些情况下，它可以更高效地处理大规模数据集。 主要特征： 复杂度O(N)： 小波变换的复杂性通常是线性的，因此它适用于大型数据集。 发现不同比例的任意形状的簇： 小波变换可以检测不同比例和形状的聚类簇，因为它在多个尺度上分析数据。 对噪声和输入次序不敏感： 小波变换具有一定的噪声鲁棒性，可以有效地处理一些噪声数据。此外，它对输入数据的次序不敏感，因此对于不同排列的数据点也能产生相似的结果。 只能应用与低维度的数据。 第五章 网络事件检测与跟踪 一、网络舆情概述 概念：网络舆情是指在互联网背景之下，众多网民关于社会（现实社会、虚拟社会）各种现象、问题所表达的信念、态度、意见和情绪表现的总和，或简言之为网络舆论和民情。 网络舆情的存活时间：大多集中在两周以内，如果回应不当，可能持续到21天左右。 网络舆情监控和预警 发生期 发酵期 发展期 高涨期 回落期 反馈期 网络舆情的特点： 直接性 网络舆情不像报纸、杂志和电视，要经过报选题、采访、编辑、审稿、发布或者播放等几个环节，而网民发帖，就没有中间环节，很直接，随意性很强，网络舆情发生以后，网民可以直接通过网站论坛、微信、QQ空间、博客等载体立即发表意见 突发性 就是无法预测，突然发生，网络舆论的形成往往非常迅速,一个热点事件的存在加上一种情绪化的意见,就可以成为点燃一片舆论的导火索。 偏差性 就是所表达的观点，与实际不符合，由于发言者身份隐蔽，并且缺少规则限制和有效监督，网络自然成为一些网民发泄情绪的空间。 网络舆情危机的传播路径 组成部分： 信源：事件。 社会问题、个人意见、重大事件、社会心理、意见领袖引导是当今网络舆情的五大因素。 网民：网络舆情的传播者。 网民是多种情绪、态度和意见的持有者, 其核心是社会政治态度。网民通过网络发表舆情言论成为引导和影响舆论的重要力量。网民是对民生、公民权利、公共治理最敏感、最敢言也最擅说话的人群，“网络舆论”可作为现实民意的风向标和参照系。 违法违规网络公关手段——“网络水军”、“网络推手”、“灌水公司”、“删帖公司”、“投票公司”等形形色色的非法网络公关机构，利用不正当手段打击竞争对手、歪曲捏造事实进行敲诈勒索、通过话题炒作制造虚假网络民意牟利、从事私下交易牟取非法利益等。 二、网络谣言 定义：广义：社会中出现并流传的未经官方公开证实或者已经被官方证伪了的信息。狭义：指没有事实根据的或凭空虚构的虚假信息。 网络谣言的传播速度更快、周期更短、波及范围更广、表现形式更多样、隐蔽性更强。 网络谣言的类型： 政治谣言 经济谣言 军事谣言 社会民生谣言 自然现象谣言 生成原因： 外部环境：国际局势错综复杂，政治谣言作为各方博弈的“攻心利器” ，在舆论场上屡见不鲜。 社会治理中也不可避免地出现了某些诸如“落实依法行政不力”等公共管理失范现象，群体焦虑情绪易被放大。 商业竞争激烈。 媒体行业规范，部分媒体为在市场竞争中取得优势，未能严守新闻职业操守及行业规范， 导致报道失实。 公民素养也是决定网络谣言产生及发酵程度的重要原因之一，即涉事主体信息公开程度越高，公民素养越高（识别信息真实性的能力越强），谣言产生的可能性就越小，谣言强度越弱。 谣言的强度=事件的重要性×事件信息的模糊性÷公众批判能力。 鉴别方法： 发布主体：信息转手次数越多，越容易失真，一手信源更容易辨别真伪。 信息内容：时间地点人物清晰可回溯，信源多元均衡，核查物证，内容前后无矛盾。 三、网络水军 定义：传统的网络“水军”是指以获取收益为主要诉求，受雇于公关公司或者营销公司，在短时间内通过大量发帖、转帖、回帖等方式满足雇佣者建构舆论、制造荣誉或恶意抹黑的特定需求，是互联网时代背景与商业需求结合的产物，也是网络营销的常用手段之一。 随着人工智能等计算机技术的发展，机器人“水军”也应运而生。 运作模式： 需求方：企业电商等 中介方：公关公司、网络推广公司 服务提供方：社会上闲散人员 危害： 助推谣言发生 制造大量网络噪声 干扰正常社会生产秩序 产生违法犯罪行为 类型： 营销型 公关型 抹黑型 检测方法： 文本内容特征： 有强烈的情感倾向，水军群体活动多以评论转发点赞为主，主动发帖少，内容包含大量商业广告和垃圾信息 账号信息特征： 账号创建时间短，名称随机，活动时间集中 用户关系特征： 正常用户的常规活动会有与亲朋好友之间的互关、互动；而水军账号多是单向的，大范围关注正常账号但是回关概率低，并且关系紧密度低。 四、话题检测与跟踪 报道（Story）：指新闻文章或者新闻电视广播中的片段，至少包含一个完整的句子。通常情况下，一篇报道只描述一个话题，但是也有些报道涉及多个话题。 事件（Event）：事件是指发生在特定时间和地点的事情，涉及了某些人和物，并且可能产生某些必然的结果。 话题（topic）：一个话题由一个种子事件或活动以及与其直接相关的事件或活动组成。因此，也可以认为话题是一个相关事件的集合，或者是若干对某事件相关报道的集合。 主题（subject）：一类事件或话题的概括，它涵盖多个类似的具体事件，或者根本不涉及任何具体的事件，主题比话题的含义更广。 话题检测与跟踪（topic detection and tracking,TDT）任务： 报道切分（SST） 一段报道可能包括多条新闻，需要切分 话题检测（TD） 将讨论同一个话题的报道聚到同一个桶（bin）中，bin的创建为无监督。 设计一个善于检测和识别所有话题的检测模型，并根据这一模型检测陆续到达的报道流，从中鉴别最新的话题；同时还需要根据已经识别到的话题，收集后续与其相关的报道。 首次报道检测任务（FSD） 检测什么时候出现了一个新话题，这个话题以前没有报道过。 FSD与TD面向的问题基本类似，但是 FSD 输出的是一篇报道，而TD输出的是一类相关于某一话题的报道集合。 话题跟踪（TT） 给定某个话题的数篇相关报道，TT识别数据流中讨论该话题的后续报道。 关联检测（LD） 给定两篇报道，判断其是否属于同一个话题。 4.1 话题表示与关联检测 TDT（Topic Detection and Tracking） 话题表示模型主要采用文本表示模型。常用的模型分为向量空间模型和语言模型。 1. 向量空间模型 这种模型基于词袋表示法，将文本中的词汇映射到高维向量空间中，其中每个维度对应一个词汇。 常见的向量化方法包括词袋模型（Bag of Words, BoW）、TF-IDF（Term Frequency-Inverse Document Frequency）等。 通过计算文本之间的向量相似度（如余弦相似度），可以用来衡量文本之间的关联程度。 TDT中通常涉及的三个不同方面的相似性，以及它们对应的对象： 报道与报道的相似度：这表示在TDT任务中，要评估两篇不同报道之间的相似性程度。这可以帮助确定两篇报道是否涉及相似的话题或事件。 话题与报道的相似度：这表示要评估某个特定话题或主题与一篇报道之间的相似性。这可以用来确定一篇报道是否与某个特定话题相关。 话题与话题的相似度：这表示要评估两个不同话题或主题之间的相似性程度。这有助于识别话题之间的关联或相似性，以及它们如何在不同报道中出现。 这三种相似性评估对应着不同的对象： 两篇文本：在第一个方面中，相似性是在两篇不同的报道或文本之间进行评估，以确定它们之间的相似性。 一篇文本与一个文本集合：在第二个方面中，相似性是评估一篇特定报道或文本与一个包含多篇文本的集合之间的相似性。 两个文本集合之间的相似度：在第三个方面中，相似性是用来比较两个不同的文本集合之间的相似性程度，以了解它们包含的话题或事件是否相似。 基于向量空间模型，判断两篇报道是否讨论了相同的话题或主题的步骤： 向量化报道：首先，将每篇报道表示为一个向量。这个向量通常是基于向量空间模型的表示，其中每个维度对应于文档中的一个特征或词项。这个向量可以使用词袋模型、TF-IDF权重或其他文本表示方法生成。 计算相似度：然后，使用向量余弦距离计算方法来度量两个报道向量之间的相似度。余弦距离是一种常用的相似度度量方法，它衡量了两个向量之间的夹角，从而表示它们在向量空间中的方向一致性。 设置阈值：在这个方法中，需要设定一个相似度阈值。这个阈值通常是一个预先定义的值，可以根据具体的任务和数据集进行调整。阈值的选择会影响最终的关联检测结果。 判断关联：最后，将计算得到的相似度与设定的阈值进行比较。如果两篇报道的相似度大于阈值，那么就认为它们讨论了相同的话题或主题，即它们具有关联性。反之，如果相似度小于阈值，那么就认为它们不讨论相同的话题。 一篇报道和一个话题之间的相似度计算的核心问题确实涉及向量之间的相似度。在关联检测任务中，通常有两种主要方法来计算一篇报道和一个话题之间的相似度： 计算报道与构成该话题的所有报道之间的相似度： 这种方法涉及将一篇报道与话题中的每一篇报道进行相似度计算。 对于每对报道之间的相似度计算都是一次关联检测的过程。通过比较一篇报道与话题中的每篇报道，可以确定它与话题的相关性。 通常，可以使用向量空间模型或其他文本表示方法来计算报道之间的相似度。 将话题表示成一个中心向量（话题模型）： 这种方法涉及将整个话题表示为一个中心向量，通常是话题中所有报道的平均向量或加权平均向量。 这个中心向量代表了话题的整体特征。一篇报道与话题之间的相似度可以通过计算报道向量与话题中心向量之间的相似度来确定。 这种方法的好处是减少了计算的复杂性，因为不需要对每对报道之间进行相似度计算。相反，只需要计算一次话题中心向量和一篇报道之间的相似度。 2. 语言模型 使用Kullback-Leibler（K-L）距离是一种衡量两个概率分布之间差异或相似度的方法，可以用于计算报道与话题之间的相似度，特别是当将文本表示为概率分布时。 以下是使用K-L距离计算报道与话题之间的相似度的一般步骤： 文本表示成概率分布： 首先，将报道和话题分别表示为概率分布。这可以通过词项的概率分布来实现，其中每个词项的概率表示在文本中出现的频率或权重。通常使用TF-IDF等方法来计算这些概率分布。 计算K-L距离： 使用K-L距离公式计算报道分布（P）和话题分布（Q）之间的距离，公式如下所示： 其中，x代表不同的词项，P(x)是报道中词项x的概率，Q(x)是话题中词项x的概率。 相似度度量： K-L距离计算的结果是一个非负数，表示两个分布之间的差异。为了将其转化为相似度度量，可以考虑使用以下方法之一： 可以使用K-L距离的倒数来得到相似性度量，即相似度 = 1 / (1 + K-L距离)。 也可以使用指数函数对K-L距离进行转换，例如，相似度 = exp(-K-L距离)。 阈值设定： 最后，可以根据具体任务需求，将计算得到的相似度值与预先设定的阈值进行比较。如果相似度值高于阈值，则认为报道与话题之间存在相关性，否则认为它们不相关。 需要注意的是，K-L距离在计算时要小心处理分布中的零概率问题，以及在面对长尾词汇时的平滑方法。 为了避免零概率问题，对概率值进行平滑处理： 其中GE为语料库，TDT任务中报道是按时间顺序出现的，新的报道可能会出现历史报道中未曾出现过的词项，可以把它当做该词项的先验知识。 4.2 话题检测 话题检测的目标是从连续的报道数据流中检测出新话题或者此前没有定义的话题。 系统对于话题的主题内容、发生时间和报道数量等信息是未知的，也没有可以用于学习的标注样本。 话题检测是一个无监督的学习任务，通常采用聚类算法来实现。 话题检测处理的对象是按时间排序的报道数据流，具有明确的时间顺序关系。数据流中的话题往往是动态演化的，不同的时间段话题的讨论内容可能不一样。 话题检测通常分为： 在线检测（online detection，也称为新事件检测New eventdetection：NED） 输入是实时的报道数据流，当前时刻的后续报道是不可见的，在每个新报道出现时，要求系统能够在线判断该报道是否属于一个新的事件。 回溯检测（retrospectivedetection：RED） 输入是包含所有时刻的完整数据集，要求系统离线地判断数据集中的报道所属的事件，并相应的将整个数据集分成若干个事件片段。 1. 在线话题检测 采用增量式的在线聚类算法，即单遍聚类算法（single-pass slustering） 算法的主要步骤： 处理输入报道：算法按照报道的顺序逐一处理输入的报道。每篇报道被表示为一个向量，其中的特征项可以是报道中的词或短语，特征的权重使用TF-IDF或其变体进行计算。 计算相似度：对于每篇新报道，算法计算它与所有已知话题之间的相似度。这里的相似度是指新报道与话题的中心向量或平均向量之间的相似度。 归类报道：如果相似度值高于预设的合并阈值，就将新报道归类到与其相似度最高的话题中。这意味着新报道被认为与该话题相关。如果相似度值低于分裂阈值，算法将创建一个新的类簇来表示新话题，并将新报道放入该类簇中。如果处于两者之间，则不处理。 反复执行：重复执行上述步骤，处理每篇新报道，直到所有的报道都被处理一遍。 形成扁平聚类：最终，算法将形成一个扁平聚类，其中每个类簇代表一个话题或一个相关报道的集合。簇的个数取决于合并-分裂阈值的大小，较低的阈值可能导致更多的话题被检测到，而较高的阈值可能导致合并相似的话题。 一些改进： 每篇报道的内容被表示为一个查询，查询的特征项是动态变化的，是由数据流中所有已出现文档的前n个高频词组成的。随着时间的变化，以前所有的查询表示都需要在新的特征项上更新一遍。 由于未来的报道在实时环境下是未知的，需要根据一个辅助语料c来 计算IDF，这个辅助语料要与当前检测的文本数据流属于同一个领域， 计算方法如下： 进一步的研究表明，新闻报道的时间特征有助于提高在线话题检测的性能，数据流中时间接近的报道更有可能讨论相同的话题。因此，可以在阈值模型中增加时间惩罚因子，当第j篇报道与第i个查询（i&lt;j ）进行比较时，相应的阈值定义如下： 2. 回溯话题检测 基于平均分组的层次聚类算法（GAC）是针对回溯检测（RED）的一种较好算法 GAC是一种自底向上的谈心算法，采用了分而治之的策略。能够最大化话题类簇中各新闻报道之间的平均相似度。输入为按照时间排序好的新闻文档集合，输出为层次式话题类簇结构。 算法基本流程： 初始化： 开始时，每篇文档被视为一个单独的话题类簇。 桶的划分： 将当前的话题类簇按照顺序连续划分到大小为m的\"桶\"中。这里的\"桶\"是一种临时容器，用于组织和处理话题类簇。 桶内聚类： 对每个桶中的话题类簇进行聚类操作。 聚类的过程采用自底向上的方式，即逐步合并最相似的底层话题类簇，以形成一个高层次的话题类簇。合并是基于两个类簇之间的相似度来完成的。 这个合并过程一直持续，直到达到预设的合并条件，可以是类簇数量减少的比例达到预设p，或者任何两个类簇之间的相似度值均低于一个预定义的阈值s。 桶的边界去除： 在保持各话题类簇事件顺序的前提下，去除桶的边界，将所有桶中的话题类簇汇集到一起。此时，得到了当前的类簇集合。 重复步骤2-4： 重复步骤2到步骤4，直到最顶层的话题类簇数目达到了预定的数值。 定期重新聚类： 定期对每个顶层类簇中的所有新闻文档进行重新聚类，按照前五步的方法。 4.3 话题跟踪 话题跟踪的主要任务是对特定话题进行追踪，即给定与特定话题相关的少量报道，检测出新闻报道流中与该话题相关的后续报道。 从信息检索的角度来看，话题跟踪与信息过滤有一些相似之处，因此可以借鉴信息过滤中的查询方法来进行话题跟踪。以下是如何利用信息过滤的查询方法进行话题跟踪的一般步骤： 建立查询器： 首先，需要建立一个查询器，该查询器用于表示待跟踪的话题。这个查询器的构建是基于话题的训练语料。训练语料包括一些已知与待跟踪话题相关的报道作为正例样本，以及其他不相关的报道作为负例样本。 查询器的目标是捕捉与待跟踪话题相关的关键词、短语或特征。 查询器的构件方法有两种：基于向量空间模型、基于语言模型 计算相似度： 对于每篇后续报道，使用建立的查询器来计算查询器与报道之间的相似度。这可以使用不同的相似性度量方法来实现，例如余弦相似度。 相似度阈值： 设定一个相似度阈值，用于判断报道是否属于待跟踪的话题。如果报道与查询器的相似度高于设定的阈值，那么可以认为这篇报道与待跟踪话题相关。 跟踪与筛选： 随着新报道的到来，计算它们与查询器的相似度，并与相似度阈值进行比较。如果相似度高于阈值，将报道纳入待跟踪话题的跟踪范围。否则，可以将其排除，认为不属于该话题。 实时更新： 设定一个查询器调整阈值，当报道相似度大于查询器调整阈值时，重构查询器以吸收该话题的重要特征，定期或实时地更新查询器，以适应话题的变化和演化。 KNN算法基本步骤： 计算待测数据点与所有训练数据的距离； 距离值递增排序； 选出前K个最小距离； 统计这K个距离值所对应的标签的频数； 频数最大的标签即为预测类别。 五、社交网络突发事件检测 社交媒体特征： 微博短文本限制：140个字 向量稀疏 特征词的文档频数分布是一个长尾分布，大部分的特征词都只在较少的微博中出现 突发特征检测方法： 假设检验方法： 这种方法假设在一个给定的时间窗口内，特征词的生成概率服从正态分布。 特征词的频率如果大于某个阈值，而且在该时间窗口内处于突发状态的概率小于5%，则被认为是突发特征词。 引入能量值方法： 这个方法引入了能量值的概念，考虑了频率和发帖者的权威度。 根据过去几个时间窗口内特征的权重值计算当前窗口内的能量值，增长速度越大，能量值越大。 使用监督或无监督方法来根据能量值判断是否为突发特征词。 Kleinberg的方法： Kleinberg提出了一种使用隐马尔科夫模型（HMM）来表示特征词生成过程的方法。 特征词在每个时间窗口内都处于一个状态，并以相应的概率被生成。 突发状态对应于高频率，状态之间的转移需要付出代价。 通过Viterbi算法求解HMM，确定最可能的状态序列，从而检测突发特征词。","link":"/2023/12/15/%E7%BD%91%E7%BB%9C%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A82/"}],"tags":[{"name":"python","slug":"python","link":"/tags/python/"},{"name":"大二上","slug":"大二上","link":"/tags/%E5%A4%A7%E4%BA%8C%E4%B8%8A/"},{"name":"密码学","slug":"密码学","link":"/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"期末复习","slug":"期末复习","link":"/tags/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/"},{"name":"大二下","slug":"大二下","link":"/tags/%E5%A4%A7%E4%BA%8C%E4%B8%8B/"},{"name":"秘密共享","slug":"秘密共享","link":"/tags/%E7%A7%98%E5%AF%86%E5%85%B1%E4%BA%AB/"},{"name":"竞赛","slug":"竞赛","link":"/tags/%E7%AB%9E%E8%B5%9B/"},{"name":"数据库","slug":"数据库","link":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"课堂笔记","slug":"课堂笔记","link":"/tags/%E8%AF%BE%E5%A0%82%E7%AC%94%E8%AE%B0/"},{"name":"大三上","slug":"大三上","link":"/tags/%E5%A4%A7%E4%B8%89%E4%B8%8A/"},{"name":"网络内容安全","slug":"网络内容安全","link":"/tags/%E7%BD%91%E7%BB%9C%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8/"}],"categories":[]}